[
  {
    "objectID": "posts/copulas/index.html",
    "href": "posts/copulas/index.html",
    "title": "Correlated Sampling Made Easy",
    "section": "",
    "text": "A common “gotcha” with creating simple Monte Carlo scripts in Python or Excel is the request to add correlation to the distributions being sampled. This is a difficult thing to do in Excel without using plugins like AtRisk, but in Python, correlated sampling can be performed conveniently with the use of Copulas.\nThe fundamental intuition to build when working with copulas is that all continuous distributions can be remapped to a \\(\\text{Uniform}[0, 1]\\) distribution with their Cumulative Distribution Function (CDF), and transformed back with the inverse CDF, also called the Percent Point Function (PPF). Furthermore, sampling from any distribution can be done by running uniform samples through its PPF (that is how random variate sampling is done in general).\nWith that trick in mind, we can make use of the multivariate Gaussian distribution and its CDF to easily generate correlated samples of any distributions we want, which is very useful when creating numerical simulation models."
  },
  {
    "objectID": "posts/copulas/index.html#generate-correlated-samples",
    "href": "posts/copulas/index.html#generate-correlated-samples",
    "title": "Correlated Sampling Made Easy",
    "section": "Generate Correlated Samples",
    "text": "Generate Correlated Samples\nLet’s say we have three parameters \\(a\\), \\(b\\) and \\(c\\) that we want to sample with some correlation coefficient \\(\\rho\\). Before we worry about the specifics of those distributions, we can sample from a multivariate Gaussian with that correlation structure.\n\n\nCode\n# Number of samples\nn = 2500\n\n# Correlation matrix\nrho = 0.80\ncorr = np.array([\n    [1.0, rho, rho],\n    [rho, 1.0, rho],\n    [rho, rho, 1.0],\n])\n\n# Mean vector (zeros with standard mv normal)\nmu = [0.0, 0.0, 0.0]\n\n# Generate samples with dims (n, parameter)\nsamples = np.random.multivariate_normal(\n    mean=mu,\n    cov=corr,\n    size=n\n)\n\n\nThis generates 2500 samples from the standard multivariate Gaussian distribution with correlation \\(\\rho = 0.8\\).\n\n\nCode\ndf = pd.DataFrame({\n    \"a\": samples[:, 0],\n    \"b\": samples[:, 1],\n    \"c\": samples[:, 2],\n})\n\ndef create_pairplot(df: pd.DataFrame, color: str = \"dodgerblue\") -&gt; alt.RepeatChart:\n    return (\n        alt.Chart(df)\n        .mark_circle(\n            color=color,\n            opacity=0.25\n        )\n        .encode(\n            alt.X(\n                alt.repeat(\"column\"), \n                type=\"quantitative\", \n                scale=alt.Scale(zero=False)\n            ),\n            alt.Y(\n                alt.repeat(\"row\"), \n                type=\"quantitative\", \n                scale=alt.Scale(zero=False)\n            )\n        )\n        .properties(\n            height=PLOT_HEIGHT / 3,\n            width=180,\n        )\n        .repeat(\n            row=list(df.columns),\n            column=list(df.columns)\n        )\n    )\n\ncreate_pairplot(df, color=\"lightcoral\")\n\n\n\n\n\n\n\n\n\n\nCode\ndf.corr()\n\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\na\n1.000000\n0.797838\n0.791882\n\n\nb\n0.797838\n1.000000\n0.802727\n\n\nc\n0.791882\n0.802727\n1.000000"
  },
  {
    "objectID": "posts/copulas/index.html#transform-to-uniform",
    "href": "posts/copulas/index.html#transform-to-uniform",
    "title": "Correlated Sampling Made Easy",
    "section": "Transform to Uniform",
    "text": "Transform to Uniform\nWe can transform those samples to \\(\\text{Uniform} [0, 1]\\) by simply passing them through the standard normal CDF.\n\n\nCode\nuniform_samples = stats.norm.cdf(df.values)\nuniform_df = pd.DataFrame({\n    \"a\": uniform_samples[:, 0],\n    \"b\": uniform_samples[:, 1],\n    \"c\": uniform_samples[:, 2],\n})\n\ncreate_pairplot(uniform_df, color=\"pink\")\n\n\n\n\n\n\n\n\nNow the sample are transformed to the uniform domain, but importantly their correlation structure has been preserved.\n\n\nCode\nuniform_df.corr()\n\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\na\n1.000000\n0.787643\n0.780945\n\n\nb\n0.787643\n1.000000\n0.793915\n\n\nc\n0.780945\n0.793915\n1.000000"
  },
  {
    "objectID": "posts/copulas/index.html#transform-to-distributions",
    "href": "posts/copulas/index.html#transform-to-distributions",
    "title": "Correlated Sampling Made Easy",
    "section": "Transform to Distributions",
    "text": "Transform to Distributions\nNow we have a bunch of uniformly distributed random samples that have our desired correlation structure. All that is left is to map the samples to our desired distributions using their respective PPFs.\nLet’s define our distributions as:\n\\[\n\\begin{align}\nA &\\sim \\text{Normal}(500, 50) \\\\\nB &\\sim \\text{Gamma}(2, 5) \\\\\nC &\\sim \\text{Beta}(5, 8)\n\\end{align}\n\\]\nNotice I’m not using normal distribtions exclusively, I can use the copula to map to whatever distributions I want. Note only that that the more skewed and kurtotic the distributions, the more warping will occur in their correlations out the other end. There are other types of Copula that can handle this better than the Gaussian Copula used here.\n\n\nCode\ndist_df = pd.DataFrame({\n    \"a\": stats.norm(500, 25).ppf(uniform_df[\"a\"]),\n    \"b\": stats.gamma(a=2, scale=5, loc=0).ppf(uniform_df[\"b\"]),\n    \"c\": stats.beta(5, 8).ppf(uniform_df[\"c\"])\n})\n\ncreate_pairplot(dist_df, color=\"orange\")\n\n\n\n\n\n\n\n\n\n\nCode\ndist_df.corr()\n\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\na\n1.000000\n0.761590\n0.791112\n\n\nb\n0.761590\n1.000000\n0.773994\n\n\nc\n0.791112\n0.773994\n1.000000"
  },
  {
    "objectID": "posts/copulas/index.html#comparison-with-uncorrelated-sampling",
    "href": "posts/copulas/index.html#comparison-with-uncorrelated-sampling",
    "title": "Correlated Sampling Made Easy",
    "section": "Comparison with Uncorrelated Sampling",
    "text": "Comparison with Uncorrelated Sampling\nIf we skip the Copula and just sample from our distributions, we get this:\n\n\nCode\nunc_dist_df = pd.DataFrame({\n    \"a\": stats.norm(500, 25).rvs(size=n),\n    \"b\": stats.gamma(a=2, scale=5, loc=0).rvs(size=n),\n    \"c\": stats.beta(5, 8).rvs(size=n)\n})\n\ncreate_pairplot(unc_dist_df, color=\"lightblue\")\n\n\n\n\n\n\n\n\n\n\nCode\nunc_dist_df.corr()\n\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\na\n1.000000\n0.008516\n0.034065\n\n\nb\n0.008516\n1.000000\n0.021285\n\n\nc\n0.034065\n0.021285\n1.000000\n\n\n\n\n\n\n\n\nEffect on Monte Carlo Simulation\nLet’s say that whatever these parameters represent, we want to know the result of this expression:\n\\[\ny = ab^c\n\\]\n\n\nCode\nunc_dist_df[\"y\"] = unc_dist_df[\"a\"] * unc_dist_df[\"b\"] ** unc_dist_df[\"c\"]\ndist_df[\"y\"] = dist_df[\"a\"] * dist_df[\"b\"] ** dist_df[\"c\"]\n\nunc_dist_df[\"Case\"] = \"Uncorrelated\"\ndist_df[\"Case\"] = \"Correlated\"\n\ncombined_df = pd.concat([dist_df, unc_dist_df], ignore_index=True)\n\n# Filter tail\nupper_lim = np.percentile(combined_df[\"y\"], q=99.5)\ncombined_df = combined_df[\n    combined_df[\"y\"] &lt; upper_lim\n]\n\nchart = (\n    alt.Chart(combined_df)\n    .mark_bar(opacity=0.75)\n    .encode(\n        x=alt.X(\"y:Q\", bin=alt.Bin(maxbins=50), axis=alt.Axis(title=\"y\")),\n        y=alt.Y(\n            \"count():Q\", \n            axis=alt.Axis(labels=False, ticks=False, title=None),\n            stack=False\n        ),\n        color=alt.Color(\n            \"Case:N\", \n            scale=alt.Scale(\n                domain=[\"Correlated\", \"Uncorrelated\"], \n                range=[\"orange\", \"lightblue\"]\n            ),\n            legend=alt.Legend(orient=\"top-right\", title=\"\")\n        )\n    )\n    .properties(height=PLOT_HEIGHT, width=PLOT_WIDTH)\n)\n\n\nchart.show()\n\n\n\n\n\n\n\n\n\n\nCode\npercentiles = [10, 50, 90]\n\nresult_df = pd.DataFrame({\n    \"10th\": np.round(np.percentile(dist_df[\"y\"], q=percentiles), 3),\n    \"50th\": np.round(np.percentile(unc_dist_df[\"y\"], q=percentiles), 3)\n}).T\n\n# Fix column labels\nresult_df.columns = [f\"P{p}\" for p in percentiles]\nresult_df.index = [\"Correlated\", \"Uncorrelated\"]\n\nresult_df\n\n\n\n\n\n\n\n\n\nP10\nP50\nP90\n\n\n\n\nCorrelated\n599.602\n1120.374\n2693.497\n\n\nUncorrelated\n656.650\n1051.744\n1942.401\n\n\n\n\n\n\n\nThe correlated samples show a wider uncertainty range in the result than the uncorrelated samples, as is expected. This may be an important detail to capture depending on the sensitivity of the analysis. The effect of correlation can also be quite unintuitive, so it is always worth checking the effect it has on results."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "How Much is a Coffee Worth to Me?\n\n\nA masochistic exploration of personal finance\n\n\n\nstupid\n\n\nfinance\n\n\ncoding\n\n\n\n\n\n\n\n\n\nJun 21, 2025\n\n\nNicholas Dorsch\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting Depositional Environments with Bayes\n\n\nModelling interpretation uncertainty with the Dirichlet distribution\n\n\n\ncoding\n\n\nstatistics\n\n\ngeology\n\n\n\n\n\n\n\n\n\nJun 7, 2025\n\n\nNicholas Dorsch\n\n\n\n\n\n\n\n\n\n\n\n\nAmy, Brett and the Cost of Living Crisis\n\n\nA Zero-Inflated Model of Household Spending\n\n\n\ncoding\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJun 2, 2025\n\n\nNicholas Dorsch\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelated Outcomes\n\n\n\n\n\n\ncoding\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJun 1, 2025\n\n\nNicholas Dorsch\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelated Sampling Made Easy\n\n\n\n\n\n\ncoding\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nMay 31, 2025\n\n\nNicholas Dorsch\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is this blog for?\n\n\n\n\n\n\nupdates\n\n\n\n\n\n\n\n\n\nMay 28, 2025\n\n\nNicholas Dorsch\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Nicholas Dorsch - Geoscientist/Data Analyist\nI come from a geology and geophysics background working in the oil and gas industry, but over the years have developed a passion for bayesian statistics and data science more broadly. My skillset includes a mixture of geology and geophysics workflows as well as more general purpose statistical modelling. The intersection of these things has allowed me to work on fairly unique projects during my career.\nI strongly believe that bayesian statistical models offer the most robust and flexible framework for uncertainty quantification, as well as decision making under uncertainty. For that reason they have become my career focus.\nMore recently I have developed more data engineering skills, working with database design and management. My goal is to become a “full stack” data scientist, capable of managing and deploying analytical processes end to end, to help organisations make the best use of their data.\n\n\nContact information\nbased in: Melbourne, Australia\navailability: currently a full time employee of Karoon Energy\nemail: nickjdorsch@gmail.com"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "What is this blog for?",
    "section": "",
    "text": "I’m making this blog to better organize my thoughts, my work portfolio and to develop my writing skills, mostly through technical sharing. It will probably contain writing on statistics, programming and the occasional book review."
  },
  {
    "objectID": "posts/outcomes/index.html",
    "href": "posts/outcomes/index.html",
    "title": "Correlated Outcomes",
    "section": "",
    "text": "In the previous post I explained how Copulas can be used to sample distributions with correlation. A nice extention of that logic is to simulate discrete outcomes with correlation as well.\n\n\nThe modelling of independent and identically distributed trials is easily done with the Binomial distribution. For example, we can simulate tossing a coin 50 times like this:\n\n\nCode\n# Simulate 5000 experiments, tossing a coin 50 times\ntosses = 50\nn = 5000\n\n# Assume a fair coin\np = 0.5\n\nheads = np.random.binomial(tosses, p, n)\n\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\ndef plot_disrete_histogram(df, col: str, color: str = \"dodgerblue\") -&gt; alt.Chart:\n    tick_values = list(range(0, tosses + 1, tosses // 10))\n\n    return (\n        alt.Chart(df)\n        .mark_bar(color=color, opacity=0.75)\n        .encode(\n            alt.X(\n                f\"{col}:Q\", \n                bin=alt.Bin(step=1), \n                scale=alt.Scale(domain=[0, tosses]),\n                axis=alt.Axis(values=tick_values, format=\".0f\")\n            ),\n            alt.Y(\"count():Q\")\n        )\n        .properties(\n            height=PLOT_HEIGHT,\n            width=PLOT_WIDTH\n        )\n    )\n\nplot_disrete_histogram(df,\"Heads\").show()\n\n\n\n\n\n\n\n\nThe above plot shows the number of heads thrown out of 50 total tosses, repeated 5000 times.\nThis can be made a bit more flexible by generating our own random uniform samples instead of directly using numpy’s random.binomial function:\n\n\nCode\n# Simulate 2D array of 0-1 values with dims (tosses, n)\nuniform_simdata = np.random.uniform(0, 1, size=(tosses, n))\n\n# Convert to a boolean array based on the p value\noutcomes = np.where(\n    uniform_simdata &lt; p,\n    True,\n    False\n)\n\n# Sum along the tosses axis to get total heads per experiment\nheads = np.sum(outcomes, axis=0)\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nplot_disrete_histogram(df, \"Heads\", color=\"coral\").show()\n\n\n\n\n\n\n\n\nThis might seem a bit inconvenient, but when simulating correlated events it becomes an effective way of getting the desired result.\n\n\n\nBecause we are now using uniform samples to create the outcomes, we can just feed correlated uniform samples into the process instead. This will correlate the outcomes.\nHere is a correlation of \\(0.25\\) shared across all coin tosses:\n\n\nCode\ndef create_simple_correlation_matrix(num: int, corr: float) -&gt; np.ndarray:\n    matrix = np.full((num, num), corr)\n    np.fill_diagonal(matrix, 1)\n    return matrix\n\ncorr = create_simple_correlation_matrix(num=tosses, corr=0.25)\n\n# Use a copula to generate correlated uniform samples\nsamples = stats.norm.cdf(\n    np.random.multivariate_normal(\n        mean=np.zeros(tosses),\n        cov=corr,\n        size=n\n    )\n).T\n\noutcomes = np.where(\n    samples &lt; p,\n    True,\n    False\n)\n\nheads = np.sum(outcomes, axis=0)\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nplot_disrete_histogram(df, \"Heads\", color=\"orange\").show()\n\n\n\n\n\n\n\n\nAnd another with a correlation of 0.75:\n\n\nCode\ncorr = create_simple_correlation_matrix(num=tosses, corr=0.75)\n\n# Use a copula to generate correlated uniform samples\nsamples = stats.norm.cdf(\n    np.random.multivariate_normal(\n        mean=np.zeros(tosses),\n        cov=corr,\n        size=n\n    )\n).T\n\noutcomes = np.where(\n    samples &lt; p,\n    True,\n    False\n)\n\nheads = np.sum(outcomes, axis=0)\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nplot_disrete_histogram(df, \"Heads\", color=\"yellow\").show()\n\n\n\n\n\n\n\n\nThe correlation between events has the effect of bifurcating the results, because events are firing together more often. A correlation of \\(1\\) using this method would return 0 heads or 100 heads, with nothing in between.\nThese examples are using a simple correlation matrix filled with the same value, but richer correlation structures can be modelled using covariance functions. The correlations might vary through time or space (or both), which means trials closer together may be more tightly related, for example. This would lead to sampling from a “binomial process” rather than the regular binomial distribution.\n\n\n\nThe framework outlined in this post and the previous provides a toolkit for numerically simulating many 0-dimensional problems (one where the answer is one number, not a higher dimensional array like a 2D map or 3D grid). Because these problems in simulation return a 1D array of results, I often refer to them as 1D models, but this can be a confusing term as the input space may have many dimensions – 1D refers only to the output space.\nMany real world problems boil down to what are called Zero-inflated models, where an event occurs with some probability \\(p\\), returning a random variable, or else returns zero:\n\\[\nX \\sim\n\\begin{cases}\n0, & \\text{with probability } 1 - p \\\\\nY \\sim \\mathcal{D}(\\theta), & \\text{with probability } p\n\\end{cases}\n\\]\nI’ll make things a bit more concrete with an example in my next post."
  },
  {
    "objectID": "posts/outcomes/index.html#independent-events",
    "href": "posts/outcomes/index.html#independent-events",
    "title": "Correlated Outcomes",
    "section": "",
    "text": "The modelling of independent and identically distributed trials is easily done with the Binomial distribution. For example, we can simulate tossing a coin 50 times like this:\n\n\nCode\n# Simulate 5000 experiments, tossing a coin 50 times\ntosses = 50\nn = 5000\n\n# Assume a fair coin\np = 0.5\n\nheads = np.random.binomial(tosses, p, n)\n\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\ndef plot_disrete_histogram(df, col: str, color: str = \"dodgerblue\") -&gt; alt.Chart:\n    tick_values = list(range(0, tosses + 1, tosses // 10))\n\n    return (\n        alt.Chart(df)\n        .mark_bar(color=color, opacity=0.75)\n        .encode(\n            alt.X(\n                f\"{col}:Q\", \n                bin=alt.Bin(step=1), \n                scale=alt.Scale(domain=[0, tosses]),\n                axis=alt.Axis(values=tick_values, format=\".0f\")\n            ),\n            alt.Y(\"count():Q\")\n        )\n        .properties(\n            height=PLOT_HEIGHT,\n            width=PLOT_WIDTH\n        )\n    )\n\nplot_disrete_histogram(df,\"Heads\").show()\n\n\n\n\n\n\n\n\nThe above plot shows the number of heads thrown out of 50 total tosses, repeated 5000 times.\nThis can be made a bit more flexible by generating our own random uniform samples instead of directly using numpy’s random.binomial function:\n\n\nCode\n# Simulate 2D array of 0-1 values with dims (tosses, n)\nuniform_simdata = np.random.uniform(0, 1, size=(tosses, n))\n\n# Convert to a boolean array based on the p value\noutcomes = np.where(\n    uniform_simdata &lt; p,\n    True,\n    False\n)\n\n# Sum along the tosses axis to get total heads per experiment\nheads = np.sum(outcomes, axis=0)\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nplot_disrete_histogram(df, \"Heads\", color=\"coral\").show()\n\n\n\n\n\n\n\n\nThis might seem a bit inconvenient, but when simulating correlated events it becomes an effective way of getting the desired result."
  },
  {
    "objectID": "posts/outcomes/index.html#correlated-events",
    "href": "posts/outcomes/index.html#correlated-events",
    "title": "Correlated Outcomes",
    "section": "",
    "text": "Because we are now using uniform samples to create the outcomes, we can just feed correlated uniform samples into the process instead. This will correlate the outcomes.\nHere is a correlation of \\(0.25\\) shared across all coin tosses:\n\n\nCode\ndef create_simple_correlation_matrix(num: int, corr: float) -&gt; np.ndarray:\n    matrix = np.full((num, num), corr)\n    np.fill_diagonal(matrix, 1)\n    return matrix\n\ncorr = create_simple_correlation_matrix(num=tosses, corr=0.25)\n\n# Use a copula to generate correlated uniform samples\nsamples = stats.norm.cdf(\n    np.random.multivariate_normal(\n        mean=np.zeros(tosses),\n        cov=corr,\n        size=n\n    )\n).T\n\noutcomes = np.where(\n    samples &lt; p,\n    True,\n    False\n)\n\nheads = np.sum(outcomes, axis=0)\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nplot_disrete_histogram(df, \"Heads\", color=\"orange\").show()\n\n\n\n\n\n\n\n\nAnd another with a correlation of 0.75:\n\n\nCode\ncorr = create_simple_correlation_matrix(num=tosses, corr=0.75)\n\n# Use a copula to generate correlated uniform samples\nsamples = stats.norm.cdf(\n    np.random.multivariate_normal(\n        mean=np.zeros(tosses),\n        cov=corr,\n        size=n\n    )\n).T\n\noutcomes = np.where(\n    samples &lt; p,\n    True,\n    False\n)\n\nheads = np.sum(outcomes, axis=0)\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nplot_disrete_histogram(df, \"Heads\", color=\"yellow\").show()\n\n\n\n\n\n\n\n\nThe correlation between events has the effect of bifurcating the results, because events are firing together more often. A correlation of \\(1\\) using this method would return 0 heads or 100 heads, with nothing in between.\nThese examples are using a simple correlation matrix filled with the same value, but richer correlation structures can be modelled using covariance functions. The correlations might vary through time or space (or both), which means trials closer together may be more tightly related, for example. This would lead to sampling from a “binomial process” rather than the regular binomial distribution."
  },
  {
    "objectID": "posts/outcomes/index.html#bringing-things-together",
    "href": "posts/outcomes/index.html#bringing-things-together",
    "title": "Correlated Outcomes",
    "section": "",
    "text": "The framework outlined in this post and the previous provides a toolkit for numerically simulating many 0-dimensional problems (one where the answer is one number, not a higher dimensional array like a 2D map or 3D grid). Because these problems in simulation return a 1D array of results, I often refer to them as 1D models, but this can be a confusing term as the input space may have many dimensions – 1D refers only to the output space.\nMany real world problems boil down to what are called Zero-inflated models, where an event occurs with some probability \\(p\\), returning a random variable, or else returns zero:\n\\[\nX \\sim\n\\begin{cases}\n0, & \\text{with probability } 1 - p \\\\\nY \\sim \\mathcal{D}(\\theta), & \\text{with probability } p\n\\end{cases}\n\\]\nI’ll make things a bit more concrete with an example in my next post."
  },
  {
    "objectID": "posts/copulas/index.html#summary-of-process",
    "href": "posts/copulas/index.html#summary-of-process",
    "title": "Correlated Sampling Made Easy",
    "section": "Summary of Process",
    "text": "Summary of Process\n\n\n\n\n\ngraph TD\n    corr[\"Correlation Matrix\"]\n    mvn[\"Standard MvNorm(0, corr)\"]\n\n    corr --&gt; mvn --&gt; mvn_samples\n\n    mvn_samples[\"MvNorm Samples [2500, 3]\"]\n    uniform[\"Uniform Samples [2500, 3]\"]\n\n    mvn_samples --&gt;|Normal CDF| uniform\n\n    dist_a[\"a\"]\n    dist_b[\"b\"]\n    dist_c[\"c\"]\n\n    dist_a_samples[\"a Samples [2500]\"]\n    dist_b_samples[\"b Samples [2500]\"]\n    dist_c_samples[\"c Samples [2500]\"]\n\n    dist_a ---&gt;|PPF| dist_a_samples\n    dist_b ---&gt;|PPF| dist_b_samples\n    dist_c ---&gt;|PPF| dist_c_samples\n\n    uniform --&gt; dist_a_samples\n    uniform --&gt; dist_b_samples\n    uniform --&gt; dist_c_samples"
  },
  {
    "objectID": "posts/spending/index.html",
    "href": "posts/spending/index.html",
    "title": "Amy, Brett and the Cost of Living Crisis",
    "section": "",
    "text": "To make the processes outlined in the previous two posts more concrete, the following is the story of a young couple trying to manage a household budget. I’ll use an aggregated zero-inflated model of spending to simulate monthly expenses."
  },
  {
    "objectID": "posts/spending/index.html#audrey",
    "href": "posts/spending/index.html#audrey",
    "title": "Audrey, Brett and the Cost of Living Crisis",
    "section": "Audrey",
    "text": "Audrey\n\nAudrey is a freelancer and goes to work on 85% of days, where she spends no money\nOn days she is not working, she spends anywhere between $20 and $150\n\n\\[\nA \\sim\n\\begin{cases}\n0, & \\text{with probability } 0.15 \\\\\nC_A \\sim \\text{PERT}(20, 50, 150), & \\text{with probability } 0.85\n\\end{cases}\n\\]"
  },
  {
    "objectID": "posts/spending/index.html#brett",
    "href": "posts/spending/index.html#brett",
    "title": "Amy, Brett and the Cost of Living Crisis",
    "section": "Brett",
    "text": "Brett\n\nBrett is unemployed, and on a given day has a 70% chance of spending\nWhen he spends, usually gambling, he makes a real impact, spending anywhere between $20 on a cheeky bet to $1500 when “we’re on here…”\nOn days Amy is home, Brett tends not to gamble, but will still place bets while on the loo, out of Amy’s sight\nThe amount Brett bets depends on Amy’s spending… if she is leaving money on the table, he tends to use it, but if she’s making big purchases he keeps his bets smaller.\n\n\\[\nB \\sim\n\\begin{cases}\n0, & \\text{with probability } 0.3 \\\\\nC_B \\sim \\text{PERT}(20, 20, 1500), & \\text{with probability } 0.7\n\\end{cases}\n\\]\nThe cost of household essentials is \\(\\$7500\\) a month. So the total monthly expenses \\(C_{\\text{total}}\\) can be given by:\n\\[\nC_{\\text{total}} = 7500 + (30(A + B))\n\\]"
  },
  {
    "objectID": "posts/spending/index.html#spending-days",
    "href": "posts/spending/index.html#spending-days",
    "title": "Amy, Brett and the Cost of Living Crisis",
    "section": "Spending Days",
    "text": "Spending Days\nFirst I’ll simulate whether or not Amy and Brett spend money on a given day, and by extension their spending days in each month:\n\n\nCode\n# Simulate a month of spending days (True or False) for Amy and Brett\ndays = 30\nn = 2500\n\np_a = 0.15\np_b = 0.7\n\n# Negative correlation between Amy and Brett spending days\nrho = -0.95\nspend_corr_matrix = np.array([\n    [1.0, rho],\n    [rho, 1.0],\n])\n\n# Use a Copula to generate correlated spending days for the month\nuniform_simdata = stats.norm.cdf(\n    np.random.multivariate_normal(\n        mean=[0, 0],\n        cov=spend_corr_matrix,\n        size=(n, days)\n    )\n)\n\n# Map to binary outcomes (True = spend, False = no spend)\nspending_days = np.where(\n    uniform_simdata &lt; [p_a, p_b],\n    True,\n    False\n)\n\ndf = pd.DataFrame({\n    \"Amy\": np.sum(spending_days[:, :, 0], axis=1),\n    \"Brett\": np.sum(spending_days[:, :, 1], axis=1),\n})\nflat_df = df.melt(var_name=\"Person\", value_name=\"Spending Days\")\n\ndef plot_disrete_histogram(df, col: str, color: str = \"dodgerblue\") -&gt; alt.Chart:\n    tick_values = list(range(0, days + 1))\n    return (\n        alt.Chart(df)\n        .mark_bar(color=color, opacity=0.5)\n        .encode(\n            alt.X(\n                f\"{col}:Q\", \n                bin=alt.Bin(step=1), \n                scale=alt.Scale(domain=[0, days]),\n                axis=alt.Axis(values=tick_values, format=\".0f\"),\n            ),\n            alt.Y(\n                \"count():Q\", \n                stack=None,\n                title='',\n                axis=alt.Axis(\n                    labels=False, \n                    ticks=False, \n                    grid=False, \n                    domain=False\n                )\n            ),\n            alt.Color(\n                \"Person:N\",\n                legend=alt.Legend(orient=\"top-right\", title=\"\")\n            ),\n        )\n        .properties(\n            title=\"Number of Days Money Spent in Month\",\n            height=PLOT_HEIGHT,\n            width=PLOT_WIDTH\n        )\n    )\n\nplot_disrete_histogram(flat_df, \"Spending Days\").show()\n\n\n\n\n\n\n\n\nSince Amy is often at work, she spends money on less days during the month. Brett, on the other hand is at home, in his element, making legendary bets.\nIt’s not obvious from the plot above, but Brett is less likely to gamble on days that Amy is at home. A 2D plot can show this better:\n\n\nCode\ntick_values = list(range(0, days + 1, 2))\nchart = (\n    alt.Chart(df)\n    .mark_rect()\n    .encode(\n        alt.X(\n            \"Amy:Q\", \n            bin=alt.Bin(maxbins=days // 2),\n            scale=alt.Scale(domain=[0, days]),\n            axis=alt.Axis(values=tick_values)\n        ),\n        alt.Y(\n            \"Brett:Q\", \n            bin=alt.Bin(maxbins=days // 2),\n            scale=alt.Scale(domain=[0, days]),\n            axis=alt.Axis(values=tick_values)\n        ),\n        alt.Color(\n            \"count():Q\",\n            scale=alt.Scale(scheme=\"oranges\"),\n            legend=None\n\n        )\n    )\n    .properties(\n        title=\"Amy vs Brett Spending Days in Month\",\n        height=PLOT_HEIGHT,\n        width=PLOT_WIDTH\n    )\n)\n\nchart.show()\n\n\n\n\n\n\n\n\nThere is a negative trend between Amy’s days at home and Brett’s spending. This is because on months where Amy isn’t getting as much work, she is at home and Brett has less opportunities to gamble. However, on months where she is working a lot, Brett is at home alone, turning the place into his personal casino."
  },
  {
    "objectID": "posts/spending/index.html#amount-spent",
    "href": "posts/spending/index.html#amount-spent",
    "title": "Amy, Brett and the Cost of Living Crisis",
    "section": "Amount Spent",
    "text": "Amount Spent\nNext, I’ll simulate how much Amy and Brett each spend on a given day. I’ll use the PERT distribution, as it is a convenient and flexible choice for subjective models like this.\n\n\nCode\nclass PERT:\n    def __init__(self, minimum: float, mode: float, maximum: float):\n        self.minimum = minimum\n        self.mode = mode\n        self.maximum = maximum\n\n        self.alpha = (\n            (4 * (self.mode - self.minimum) / (self.maximum - self.minimum)) + 1\n        )\n        self.beta = (\n            (4 * (self.maximum - self.mode) / (self.maximum - self.minimum)) + 1\n        )\n\n        # Beta distribution on [0,1]\n        self.beta_dist = stats.beta(self.alpha, self.beta)\n\n    def pdf(self, x):\n        \"\"\"\n        Calculate the PDF of the PERT distribution at x.\n        \"\"\"\n        if x &lt; self.minimum or x &gt; self.maximum:\n            return 0.0\n\n        # Scale x to [0,1]\n        x_scaled = (x - self.minimum) / (self.maximum - self.minimum)\n        return self.beta_dist.pdf(x_scaled) / (self.maximum - self.minimum)\n\n    def rvs(self, shape: int | tuple[int] = 1) -&gt; np.ndarray:\n        \"\"\"\n        Generate random variates of given size from the PERT distribution.\n        \"\"\"\n        samples_scaled = self.beta_dist.rvs(size=shape)\n        # Scale samples back to [minimum, maximum]\n        return self.minimum + samples_scaled * (self.maximum - self.minimum)\n\n    def ppf(self, q):\n        \"\"\"\n        Percent-point function (inverse CDF) at q of the PERT distribution.\n        \"\"\"\n        x_scaled = self.beta_dist.ppf(q)\n        return self.minimum + x_scaled * (self.maximum - self.minimum)\n\n    def plot(\n        self, \n        color: str = \"dodgerblue\", \n        title: str = \"PERT PDF\",\n        height=PLOT_HEIGHT, \n        width=PLOT_WIDTH\n    ) -&gt; alt.Chart:\n        x_vals = np.linspace(self.minimum, self.maximum, 200)\n        pdf_vals = [self.pdf(x) for x in x_vals]\n\n        df = pd.DataFrame({\n            'x': x_vals,\n            'pdf': pdf_vals\n        })\n\n        chart = (\n            alt.Chart(df)\n            .mark_area(line={\"color\": color}, color=color, opacity=0.5)\n            .encode(\n                x=alt.X('x', title='$'),\n                y=alt.Y(\n                    'pdf', \n                    title='',\n                    axis=alt.Axis(\n                        labels=False, \n                        ticks=False, \n                        grid=False, \n                        domain=False\n                    )\n                )\n            )\n            .properties(\n                width=width,\n                height=height,\n                title=title\n            )\n        )\n        return chart\n\n\n\nAmy\n\\[\nC_A \\sim \\text{PERT}(20, 50, 150)\n\\]\nAmy is not a big spender. She is usually at work, but when she has a day off she never spends more than $150 during the day, and usually hovers around the $50 mark.\n\n\nCode\nspend_a = PERT(20, 50, 150)\nspend_a.plot(title=\"Amy's Daily Spending\").show()\n\n\n\n\n\n\n\n\n\n\nBrett\n\\[\nC_B \\sim \\text{PERT}(20, 20, 1500)\n\\]\nBrett’s habits are a lot more volatile. He is always at home, and overall much more likely to spend. When he does spend, it is anything between a small $20 bet to significant $1500 wagers.\n\n\nCode\nspend_b = PERT(20, 20, 1500)\nspend_b.plot(title=\"Brett's Daily Spending\", color=\"orange\").show()"
  },
  {
    "objectID": "posts/spending/index.html#month-of-spending",
    "href": "posts/spending/index.html#month-of-spending",
    "title": "Amy, Brett and the Cost of Living Crisis",
    "section": "Month of Spending",
    "text": "Month of Spending\nUsing the spending day simulation in combination with correlated sampling of the two spending distributions, I can estimate the monthly spending of each member of the household:\n\n\nCode\n# Negative correlation between Amy and Brett spending\nrho = -0.95\nspend_corr_matrix = np.array([\n    [1.0, rho],\n    [rho, 1.0],\n])\n\n# Use a Copula to generate correlated spending days for the month\nuniform_simdata = stats.norm.cdf(\n    np.random.multivariate_normal(\n        mean=[0, 0],\n        cov=spend_corr_matrix,\n        size=(n, days)\n    )\n)\n\nsimdata_a = spend_a.ppf(uniform_simdata[:, :, 0])\nsimdata_b = spend_b.ppf(uniform_simdata[:, :, 1])\n\n# Mask with array of spending days\nsimdata_a = np.where(\n    spending_days[:, :, 0],\n    simdata_a,\n    0\n)\nsimdata_b = np.where(\n    spending_days[:, :, 1],\n    simdata_b,\n    0\n)\n\ndf = pd.DataFrame({\n    \"Amy\": np.sum(simdata_a, axis=1),\n    \"Brett\": np.sum(simdata_b, axis=1),\n})\n\nflat_df = df.melt(var_name=\"Person\", value_name=\"Spending\")\ndf[\"TOTAL\"] = df[\"Amy\"] + df[\"Brett\"]\n\n\nPlotting to show Amy and Brett’s spending separately paints a pretty clear picture:\n\n\nCode\nchart = (\n    alt.Chart(flat_df)\n    .mark_bar(opacity=0.5)\n    .encode(\n        alt.X(\n            \"Spending:Q\", \n            bin=alt.Bin(maxbins=100)\n        ),\n        alt.Y(\"count():Q\", stack=None),\n        alt.Color(\n            \"Person:N\",\n            legend=alt.Legend(orient=\"top-right\", title=\"\")\n        )\n    )\n    .properties(\n        width=PLOT_WIDTH,\n        height=PLOT_HEIGHT,\n        title=\"Household Monthly Spending\"\n    )\n)\n\nchart.show()\n\n\n\n\n\n\n\n\nBrett has a problem.\nThe negative correlation between Amy and Brett’s spending is in the model, though it doesn’t show up very strongly in the aggregated monthly spending:\n\n\nCode\nchart = (\n    alt.Chart(df)\n    .mark_point()\n    .encode(\n        alt.X(\"Amy:Q\"),\n        alt.Y(\"Brett:Q\"),\n        alt.Color(\n            \"TOTAL:Q\",\n            legend=alt.Legend(orient=\"top-right\")\n        )\n    )\n    .properties(\n        width=PLOT_WIDTH,\n        height=PLOT_HEIGHT,\n        title=\"Amy vs Brett Monthly Spending\"\n    )\n)\n\nchart.show()"
  },
  {
    "objectID": "posts/spending/index.html#overall-spend",
    "href": "posts/spending/index.html#overall-spend",
    "title": "Audrey, Brett and the Cost of Living Crisis",
    "section": "Overall Spend",
    "text": "Overall Spend\nNow I can plug the above simulated numbers into the formula:\n\\[\nC_{\\text{total}} = 6000 + (30(A + B))\n\\]\nto arrive at the final monthly total.\nAssuming Audrey makes $500 per day worked after tax, I can also compute a monthly net income.\n\n\nCode\ndef calculate_total_monthly_expenses(\n    audrey_spend, \n    brett_spend, \n    base_expense: float = 6000\n) -&gt; np.ndarray:\n    audrey_total_spend = np.sum(audrey_spend, axis=1)\n    brett_total_spend = np.sum(brett_spend, axis=1)\n\n    return base_expense + audrey_total_spend + brett_total_spend\n\ntotal_expenses = calculate_total_monthly_expenses(\n    audrey_spend=simdata_a,\n    brett_spend=simdata_b,\n    base_expense=6000\n)\n\nincome_per_day = 500\nincome = np.sum(\n    np.where(\n        ~spending_days[:, :, 0],\n        income_per_day, \n        0\n    ),\n    axis=1\n)\n\ndf = pd.DataFrame({\n    \"Expenses\": total_expenses,\n    \"Income\": income,\n    \"Net Income\": income - total_expenses\n})\nflat_df = df.melt(var_name=\"Total\", value_name=\"Amount\")\nflat_df = flat_df.sample(5000)\n\nchart = (\n    alt.Chart(flat_df)\n    .mark_bar(opacity=0.5)\n    .encode(\n        alt.X(\"Amount:Q\", bin=alt.Bin(maxbins=50)),\n        alt.Y(\n            \"count():Q\", \n            stack=None,\n            title='',\n            axis=alt.Axis(\n                labels=False, \n                ticks=False, \n                grid=False, \n                domain=False\n            ),\n        ),\n        alt.Color(\n            \"Total:N\",\n            legend=alt.Legend(orient=\"top-left\", title=\"\")\n        ),\n        alt.Order(\"Total:N\")\n    )\n    .properties(\n        width=PLOT_WIDTH,\n        height=PLOT_HEIGHT,\n        title=\"Monthly Budget\"\n    )\n)\n\nchart.show()\n\n\n\n\n\n\n\n\nUsing the Net Income result, I can estimate the probability that the household will lose money in a given month:\n\n\nCode\np_lose_money = float(np.round(np.mean(df[\"Net Income\"] &lt;= 0) * 100, 2))\n\n\nThis tells us that the household has a 23.12% chance of losing money each month, and that Audrey should probably get a divorce."
  },
  {
    "objectID": "posts/spending/index.html#overall-budget",
    "href": "posts/spending/index.html#overall-budget",
    "title": "Amy, Brett and the Cost of Living Crisis",
    "section": "Overall Budget",
    "text": "Overall Budget\nNow I can plug the above simulated numbers into the formula:\n\\[\nC_{\\text{total}} = 7500 + (30(A + B))\n\\]\nto arrive at the final monthly total.\nAssuming Amy makes $550 per day worked after tax, I can also compute a monthly net income.\n\n\nCode\ndef calculate_total_monthly_expenses(\n    audrey_spend, \n    brett_spend, \n    base_expense: float = 7500\n) -&gt; np.ndarray:\n    audrey_total_spend = np.sum(audrey_spend, axis=1)\n    brett_total_spend = np.sum(brett_spend, axis=1)\n\n    return base_expense + audrey_total_spend + brett_total_spend\n\ntotal_expenses = calculate_total_monthly_expenses(\n    audrey_spend=simdata_a,\n    brett_spend=simdata_b,\n    base_expense=7500\n)\n\nincome_per_day = 550\nincome = np.sum(\n    np.where(\n        ~spending_days[:, :, 0],\n        income_per_day, \n        0\n    ),\n    axis=1\n)\n\ndf = pd.DataFrame({\n    \"Expenses\": total_expenses,\n    \"Income\": income,\n    \"Net Income\": income - total_expenses\n})\nflat_df = df.melt(var_name=\"Total\", value_name=\"Amount\")\nflat_df = flat_df.sample(5000)\n\nchart = (\n    alt.Chart(flat_df)\n    .mark_bar(opacity=0.5)\n    .encode(\n        alt.X(\"Amount:Q\", bin=alt.Bin(maxbins=50)),\n        alt.Y(\n            \"count():Q\", \n            stack=None,\n            title='',\n            axis=alt.Axis(\n                labels=False, \n                ticks=False, \n                grid=False, \n                domain=False\n            ),\n        ),\n        alt.Color(\n            \"Total:N\",\n            legend=alt.Legend(orient=\"top-left\", title=\"\")\n        ),\n        alt.Order(\"Total:N\")\n    )\n    .properties(\n        width=PLOT_WIDTH,\n        height=PLOT_HEIGHT,\n        title=\"Monthly Budget\"\n    )\n)\n\nchart.show()\n\n\n\n\n\n\n\n\nUsing the Net Income result, I can estimate the probability that the household will lose money in a given month:\n\n\nCode\np_lose_money = float(np.round(np.mean(df[\"Net Income\"] &lt;= 0) * 100, 2))\n\n\nThis tells us that the household has a 30.48% chance of losing money each month, and that Amy should probably get a divorce."
  },
  {
    "objectID": "posts/spending/index.html#amy",
    "href": "posts/spending/index.html#amy",
    "title": "Amy, Brett and the Cost of Living Crisis",
    "section": "Amy",
    "text": "Amy\n\nAmy is a freelancer and goes to work on 85% of days, where she spends no money\nOn days she is not working, she spends anywhere between $20 and $150\n\n\\[\nA \\sim\n\\begin{cases}\n0, & \\text{with probability } 0.15 \\\\\nC_A \\sim \\text{PERT}(20, 50, 150), & \\text{with probability } 0.85\n\\end{cases}\n\\]"
  },
  {
    "objectID": "posts/dirichlet_geo/index.html",
    "href": "posts/dirichlet_geo/index.html",
    "title": "Interpreting Depositional Environments with Bayes",
    "section": "",
    "text": "A well preserved Bouma sequence from Tabernas Basin, Spain. link"
  },
  {
    "objectID": "posts/dirichlet_geo/index.html#the-dirichlet-distribution",
    "href": "posts/dirichlet_geo/index.html#the-dirichlet-distribution",
    "title": "Interpreting Depositional Environments with Bayes",
    "section": "The Dirichlet Distribution",
    "text": "The Dirichlet Distribution\nThe Dirichlet distribution can be used to model the probability of a set of discrete possibilities—depositional environments, in this context. The Dirichlet distribution can generates samples from a “unit simplex” (a vector of numbers that add to 1), which makes it perfect for modelling our uncertainty over interpretations, where we allocate some probability to each.\nIt also has the convenient property of conjugacy, which essentially means it is very easy to mathematically update as new evidence comes in."
  },
  {
    "objectID": "posts/dirichlet_geo/index.html#applied-to-a-simple-example",
    "href": "posts/dirichlet_geo/index.html#applied-to-a-simple-example",
    "title": "Interpreting Depositional Environments with Bayes",
    "section": "Applied to a Simple Example",
    "text": "Applied to a Simple Example\nSay we are considering three depositional environments when looking at part of an outcrop:\n\nDelta\nTurbidite\nLake\n\nThe \\(\\alpha\\) parameter of the distribution can be thought of as a list of weights or “pseudocounts”, one for each possibility.\nIn this case let’s say we assign our weights as:\n\\[\n\\alpha = [20, 10, 2]\n\\]\nThis equates to us saying that we think the Delta interpretation is most plausible, although the Turbidite interpretation is a possibility, while the Lake interpretation is implausible.\nThat results in the following distribution:\n\n\nCode\ndef create_marginal_dirichlet_pdf_df(\n    alpha: list[int], \n    labels: list[str]\n) -&gt; pd.DataFrame:\n\n    x = np.linspace(0, 1, 250)\n    total = np.array(alpha).sum()\n\n    dfs = []\n    for l, a in zip(labels, alpha):\n        marginal_pdf = beta(a=a, b=(total - a)).pdf(x)\n\n        dfs.append(\n            pd.DataFrame({\n                \"Interpretation\": l,\n                \"Probability\": x,\n                \"PDF\": marginal_pdf\n            })\n        )\n\n    return pd.concat(dfs, ignore_index=True)\n\n\ndef plot_dirichlet_pdf(df: pd.DataFrame, title: str) -&gt; alt.Chart:\n    tick_values = [round(i * 0.1, 1) for i in range(11)]\n    return (\n        alt.Chart(df)\n        .mark_area(opacity=0.5)\n        .encode(\n            alt.X(\n                \"Probability:Q\", \n                scale=alt.Scale(domain=[0, 1]),\n                axis=alt.Axis(values=tick_values)\n            ),\n            alt.Y(\n                \"PDF:Q\",\n                stack=None,\n                title='',\n                axis=alt.Axis(\n                    labels=False, \n                    ticks=False, \n                    grid=False, \n                    domain=False\n                ),\n            ),\n            alt.Color(\n                \"Interpretation:N\",\n                legend=alt.Legend(orient=\"top-right\")\n            ),\n            alt.Order(\"Interpretation:N\")\n        )\n        .properties(\n            title=title,\n            height=PLOT_HEIGHT,\n            width=PLOT_WIDTH\n        )\n    )\n\nalpha = [20, 10, 2]\nlabels = [\n    \"P(Delta)\",\n    \"P(Turbidite)\",\n    \"P(Lake)\",\n]\n\ndf = create_marginal_dirichlet_pdf_df(alpha, labels)\n\nchart = plot_dirichlet_pdf(df, title=\"Interpretation Probabilities\")\nchart.show()\n\n\n\n\n\n\n\n\nThis is the distribution over the probabilities of each of the interpretations. Maybe it seems weird to have a probability distribution over probabilities, but this is actually very common in bayesian statistics. I’ll probably write a post on that at some point, but see the Aleatoric and epistemic section of this wiki page for a primer."
  },
  {
    "objectID": "posts/dirichlet_geo/index.html#incorporating-prior-information",
    "href": "posts/dirichlet_geo/index.html#incorporating-prior-information",
    "title": "Interpreting Depositional Environments with Bayes",
    "section": "Incorporating Prior Information",
    "text": "Incorporating Prior Information\nLet’s say despite our observations at this outcrop, we have strong indications nearby that we are standing in the middle of a lake deposit. Maybe the outcrop is an anomaly in an otherwise fairly confident interpretation.\nIf we encode that prior as \\(\\alpha_{\\text{prior}} = [1, 1, 50]\\), it looks like this:\n\n\nCode\nprior_alpha = np.array([1, 1, 50])\ndf = create_marginal_dirichlet_pdf_df(prior_alpha, labels)\n\nchart = plot_dirichlet_pdf(df, title=\"Prior Interpretation Probabilities\")\nchart.show()\n\n\n\n\n\n\n\n\nWe can incorporate this prior information very easily in our \\(\\alpha\\) parameter by just… adding it! It seems like a dumb trick, but this is an analytical result for updating Dirichlet distributions in what’s called a Dirichlet Multinomial conjugate model. If we are comfortable treating our vector of \\(\\alpha\\) values as “pseudocounts”, then the math checks out here.\n\\[\n\\begin{align}\n\\alpha_{\\text{prior}} &= [1, 1, 50] \\\\\n\\alpha_{\\text{posterior}} &= \\alpha_{\\text{prior}} + \\alpha \\\\\n\\alpha_{\\text{posterior}} &= [21, 11, 52]\n\\end{align}\n\\]\n\n\nCode\nposterior_alpha = prior_alpha + alpha\n\ndf = create_marginal_dirichlet_pdf_df(posterior_alpha, labels)\n\nchart = plot_dirichlet_pdf(df, title=\"Posterior Interpretation Probabilities\")\nchart.show()\n\n\n\n\n\n\n\n\nNow, the strong prior on the Lake interpretation pushes it ahead, but notice the other interpretations are still on the table as possibilities, given the evidence observed at the outcrop.\nIn my view this provides a pretty strong baseline. We have 3 important things:\n\nThe ability to encode prior beliefs over interpretations,\nThe ability to “score” interpretations based on their likelihoods under those interpretations, and\nThe ability to update probabilities objectively.\n\nThis all hinges on a point-scoring system that makes logical sense, but it seems like a good start."
  },
  {
    "objectID": "posts/dirichlet_geo/index.html#in-action",
    "href": "posts/dirichlet_geo/index.html#in-action",
    "title": "Interpreting Depositional Environments with Bayes",
    "section": "In Action",
    "text": "In Action\nI don’t have real examples to use, but let’s consider a diligent field geologist’s notes about an outcrop.\nSince Claude is already cooking, I’ve used it to dream up an example:\n\nFIELD NOTES - Station 15, Muddy Creek Section Date: June 15, 2024 Weather: Overcast, good exposure Geologist: Dr. Sarah Mitchell\n\n\nINTERVAL: 45.2 - 52.8m (Sandstone Unit C)\n\n\nDETAILED OBSERVATIONS: Trough cross-stratification: Observed in 7 separate beds, sets 0.3-1.2m thick. Parallel lamination: Very common, counted 12 distinct intervals. Massive sandstone: 3 thick beds (0.8-1.5m), clean, well-sorted. Plant debris: Abundant! Counted 15 fragments/coalified pieces on bedding planes. Ripple lamination: Present in 4 beds, mostly at tops of fining-up sequences. Fining upward sequences: Clear in 5 complete cycles, 2-3m thick each. Channel lag deposits: 2 clear examples at sequence bases, pebble lags. Mud drapes: Rare, only 1 thin example on ripple surface. Bioturbation: Moderate, 6 intervals with Skolithos-type traces.\n\n\nINTERPRETATION NOTES: This interval screams fluvial-deltaic to me. The trough cross-beds in thick sets, abundant plant material, and those beautiful fining-upward cycles. The channel lags are textbook. Probably distributary channel fill transitioning upward to delta front. Very little marine influence based on lack of mud drapes and bioturbation style.\n\n\nCONFIDENCE: High - excellent exposure, clear diagnostic features\n\nClaude is… quite good at this.\nWrapping this up into data, we can calculate the posterior \\(\\alpha\\) values using the previous table.\n\n\nCode\nfield_observations = {\n    'trough_cross_stratification': 7,\n    'parallel_lamination': 12,\n    'massive_sandstone': 3,\n    'plant_debris': 15,\n    'ripple_lamination': 4,\n    'fining_upward_sequences': 5,\n    'channel_lag_deposits': 2,\n    'mud_drapes': 1,\n    'bioturbation': 6\n}\n\n\ndef calculate_posterior_alphas(\n    obs_df: pd.DataFrame, \n    field_observations: dict[str, int]\n):\n    alpha_counts = {k: 0 for k in obs_df.columns if k != \"observation\"}\n\n    for obs, count in field_observations.items():\n        obs_row = obs_df[obs_df['observation'] == obs]\n\n        for alpha in alpha_counts.keys():\n            a = obs_row[alpha].values[0]\n            alpha_counts[alpha] += (a * count)\n\n    return alpha_counts\n\nalpha_dict = calculate_posterior_alphas(obs_df, field_observations)\n\nalpha_obs = np.array(list(alpha_dict.values()))\n\n\nAnd then plot it:\n\n\nCode\ndf = create_marginal_dirichlet_pdf_df(alpha_obs, labels)\n\nchart = plot_dirichlet_pdf(df, title=\"Field Observations: Probabilities\")\nchart.show()\n\n\n\n\n\n\n\n\nDr. Mitchell’s remarks were broadly correct, then, though the model only gives about 50% probability to the Delta interpretation."
  },
  {
    "objectID": "posts/dirichlet_geo/index.html#incorporating-other-information",
    "href": "posts/dirichlet_geo/index.html#incorporating-other-information",
    "title": "Interpreting Depositional Environments with Bayes",
    "section": "Incorporating Other Information",
    "text": "Incorporating Other Information\nIn the same way we incorporated priors before, we can use the same procedure to incorporate external sources of information. For example, the geophysicist working on this dataset may have a strong conviction that this is a turbidite deposit based on interpretation of seismic data nearby that connects to this outcrop stratigraphically:\n\\[\n\\begin{align}\n\\alpha_{\\text{geophysics}} = [1, 200, 1]\n\\end{align}\n\\]\n\n\nCode\nnew_alpha = np.array(alpha_obs) + np.array([1, 200, 1])\ndf = create_marginal_dirichlet_pdf_df(new_alpha, labels)\n\nchart = plot_dirichlet_pdf(\n    df, \n    title=\"Field Observations + Geophysics: Probabilities\"\n)\nchart.show()\n\n\n\n\n\n\n\n\nAgain, it wouldn’t be easy to calibrate these \\(\\alpha\\) values, because they are encoding subjective beliefs about the strength of evidence under different interpretations. But at least we can say that we have considered the geophysicist’s viewpoint and updated our beliefs accordingly."
  },
  {
    "objectID": "posts/coffee/index.html",
    "href": "posts/coffee/index.html",
    "title": "How Much is a Coffee Worth to Me?",
    "section": "",
    "text": "I moved to Melbourne last year, which, along with being a questionable career choice, turned an already troubling dependence on caffeine into a behavioural disorder.\nI bought a Nespresso machine—telling myself I would save money on coffee—and it has been a huge success. Now, with the help of my Nespresso machine I have more than doubled my coffee consumption. The coffee pod coffee doesn’t count, you see. Yes I’ll come down for a coffee, I haven’t had mine yet.\nThis leads me to wonder how much of my family’s future I am eroding to bean-dust, every time I head downstairs for another medium latte, thanks—the absent droning sound I make every morning between the hours of 9 and 10 am. Each milky-upper is costing me $6. So assuming I work 250 days in a year… let’s see now:\nCode\nCOFFEE_COST = 6.0\nWORKING_DAYS = 250\n\nannual_cost = COFFEE_COST * WORKING_DAYS\nIt’s $1500.0. You can check the code if you want to make sure I got that.\nI’m not devastated by that number—it’s not a small amount, but it’s not enough to pay for much in Australia, either. Maybe a less masochistic person would leave the matter there, but it begs the question of what I might be doing with that money if I wasn’t spending it all on morning browns to straighten out my morning frowns."
  },
  {
    "objectID": "posts/coffee/index.html#basic-model",
    "href": "posts/coffee/index.html#basic-model",
    "title": "How Much is a Coffee Worth to Me?",
    "section": "Basic Model",
    "text": "Basic Model\nBefore things get more complicated, let’s have a look at how these strategies perform assuming an 8% annual return on the market:\n\n\nCode\ndef create_date_array(start_date: datetime, num_months: int):\n    start = np.datetime64(start_date.replace(day=1), 'M')\n    month_array = start + np.arange(num_months)\n    return month_array.astype(\"datetime64[D]\")\n\ndef annual_to_monthly_rate(annual_rate: float) -&gt; float:\n    return (1 + annual_rate) ** (1 / 12) - 1\n\ndef calculate_compound_returns(\n    investments: np.ndarray, \n    annual_rate: float\n) -&gt; np.ndarray:\n    num_months = len(investments)\n    monthly_return = annual_to_monthly_rate(annual_rate)\n    returns = np.zeros(num_months)\n\n    for i in range(num_months):\n        months_invested = np.arange(num_months - i)\n        returns[i:] += investments[i] * (1 + monthly_return) ** months_invested\n\n    return returns\n\n\nYEARS = 20\nANNUAL_RATE = 0.08\nDAYS_IN_MONTH = 30.437      # Accounting for gap years\n\nnum_months = YEARS * 12\nmonths = np.arange(num_months)\ndates = create_date_array(datetime.today(), num_months=num_months)\n\ninvesting = np.full_like(dates, COFFEE_COST * DAYS_IN_MONTH).astype(float)\nspending = -1 * investing\nreturns = calculate_compound_returns(\n    investments=investing,\n    annual_rate=ANNUAL_RATE\n)\n\ndf = pd.DataFrame({\n    \"Date\": dates,\n    \"Coffee\": np.cumsum(spending),\n    \"Investing\": np.cumsum(investing),\n    \"Returns\": returns - np.cumsum(investing),\n}).round(2)\nflat_df = df.melt(\n    id_vars=[\"Date\"], \n    var_name=\"Case\", \n    value_name=\"$\"\n)\n\n\n\n\nCode\ndef plot_balance_area_chart(df: pd.DataFrame, title: str):\n    return (\n        alt.Chart(df)\n        .mark_area(opacity=0.75)\n        .encode(\n            alt.X(\"Date:T\", title=\"\"),\n            alt.Y(\"$:Q\"),\n            alt.Color(\n                \"Case:N\",\n                legend=alt.Legend(orient=\"top-left\")\n            ),\n            tooltip=[\n                alt.Tooltip(\"Date:T\", title=\"\"),\n                alt.Tooltip(\"$:Q\", title=\"$\", format=\"$,.2f\"),\n                alt.Tooltip(\"Case:N\", title=\"\")\n            ],\n            order=alt.Order(\"Case:N\")\n        )\n        .properties(\n            title=title,\n            height=PLOT_HEIGHT,\n            width=PLOT_WIDTH\n        )\n    )\n\nchart = plot_balance_area_chart(\n    flat_df, title=\"Comparison of Investing vs Coffee Drinking\"\n)\nchart.show()\n\n\n\n\n\n\n\n\n\n\nCode\ntotal_gains = int(df.iloc[-1][\"Investing\"] + df.iloc[-1][\"Returns\"])\ntotal_losses = int(df.iloc[-1][\"Coffee\"])\n\n\nUnless I’ve screwed up the maths here, things are already looking very grim. Nick the dribbler is down $43680 in 20 years, whereas Nick the investor is giving his kid a very nice $103557 graduation bonus.\n\nDevaluing the Future\nOk, but what about inflation, and the time value of money, and all that finance stuff? asks Nick the dribbler, pretending he isn’t an idiot.\nWell the time value of money is a bit of a flimsy concept when the present money is being spent on hot milk that will cool down in minutes and expire in a matter of days, but let’s see.\nAssuming 2.5% inflation annually, and a discount rate of 13.5%, which look like this:\n\n\nCode\ndef create_exponential_curve(\n    months: np.ndarray, \n    annual_rate: float\n) -&gt; np.ndarray:\n    monthly_rate = annual_to_monthly_rate(annual_rate)\n    return np.exp(months * monthly_rate)\n\n\nDISCOUNT_RATE = -0.135\nINFLATION_RATE = 0.025\n\ndiscount_curve = create_exponential_curve(months, DISCOUNT_RATE)\ninflation_curve = create_exponential_curve(months, INFLATION_RATE)\n\ndef plot_factor_curves(discount_curve, inflation_curve) -&gt; alt.Chart:\n    return (\n        alt.Chart(\n            pd.DataFrame({\n                \"Date\": dates,\n                \"Inflation\": inflation_curve,\n                \"Discount\": discount_curve,\n                \"Net Adjustment\": inflation_curve * discount_curve\n            })\n            .melt(\n                id_vars=[\"Date\"], \n                var_name=\"Curve\",\n                value_name=\"Factor\"\n            )\n        )\n        .mark_line()\n        .encode(\n            alt.X(\"Date:T\"),\n            alt.Y(\"Factor:Q\"),\n            alt.Color(\n                \"Curve:N\",\n                legend=alt.Legend(orient=\"top-left\"),\n            ),\n            tooltip=[\n                alt.Tooltip(\"Date:T\", title=\"\"),\n                alt.Tooltip(\"Factor:Q\", format=\".3f\"),\n                alt.Tooltip(\"Curve:N\", title=\"\")\n            ],\n        )\n        .properties(\n            height=PLOT_HEIGHT,\n            width=PLOT_WIDTH\n        )\n    )\n\nchart = plot_factor_curves(discount_curve, inflation_curve)\nchart.show()\n\n\n\n\n\n\n\n\n… Investor Nick’s returns look like this:\n\n\nCode\ndiscount_df = pd.DataFrame({\n    \"Date\": dates,\n    \"Coffee\": np.cumsum(spending) * inflation_curve * discount_curve,\n    \"Investing\": np.cumsum(investing) * inflation_curve * discount_curve,\n    \"Returns\": (returns - np.cumsum(investing)) * inflation_curve * discount_curve,\n}).round(2)\n\nflat_discount_df = discount_df.melt(\n    id_vars=[\"Date\"], \n    var_name=\"Case\", \n    value_name=\"$\"\n)\n\nchart = plot_balance_area_chart(\n    flat_discount_df, \n    title=\"(Real, Discounted at 13.5%): Comparison of Investing vs Coffee Drinking\"\n)\nchart.show()\n\n\n\n\n\n\n\n\n…So what does that mean?\nIf present me values future dollars at a discount rate of 13.5%, I am basically claiming that money on the table today can be converted into a 13.5% return in a year, through my… savvy investing strategy.\nThat means that future dollars are worth less to me (not worthless, worth less). The sooner I have those dollars, the sooner I can put them to work to make me that return.\nAnd compounding this out to 20 years from now implies that a dollar then is worth about 5% of its value to me today.\nThis applies to dollars spent as well as dollars invested—future expenses are smaller when discounted too.\nSo what discount rate could justify Nick the dribbler’s behaviour? Well, he isn’t investing that money, which means he must really, really value that 3 minute mouth experience, which would imply a very aggressive discount rate, probably in excess of 100%.\n\n\nCode\ndiscount_curve = create_exponential_curve(months, annual_rate=-1)\n\nchart = plot_factor_curves(discount_curve, inflation_curve)\nchart.show()\n\n\n\n\n\n\n\n\n\n\nCode\ndiscount_df = pd.DataFrame({\n    \"Date\": dates,\n    \"Coffee\": np.cumsum(spending) * inflation_curve * discount_curve,\n    \"Investing\": np.cumsum(investing) * inflation_curve * discount_curve,\n    \"Returns\": (returns - np.cumsum(investing)) * inflation_curve * discount_curve,\n}).round(2)\n\nflat_discount_df = discount_df.melt(\n    id_vars=[\"Date\"], \n    var_name=\"Case\", \n    value_name=\"$\"\n)\n\nchart = plot_balance_area_chart(\n    flat_discount_df, \n    title=\"(Real, Discounted at 100%): Comparison of Investing vs Coffee Drinking\"\n)\nchart.show()\n\n\n\n\n\n\n\n\nSo, any money beyond a year time horizon is worth jack shit to Nick the dribbler. He lives by the froth, dies by the froth, neglects his financial future by the froth.\nFrom a maximize money standpoint this isn’t rational, and realistically speaking I would expect an individual to have a discount rate of a little over some risk-free return—like treasury bonds at around 4.5%—up to around 9%, depending on their risk tolerance. Here’s 6%:\n\n\nCode\ndiscount_curve = create_exponential_curve(months, annual_rate=-0.06)\ndiscount_df = pd.DataFrame({\n    \"Date\": dates,\n    \"Coffee\": np.cumsum(spending) * inflation_curve * discount_curve,\n    \"Investing\": np.cumsum(investing) * inflation_curve * discount_curve,\n    \"Returns\": (returns - np.cumsum(investing)) * inflation_curve * discount_curve,\n}).round(2)\n\nflat_discount_df = discount_df.melt(\n    id_vars=[\"Date\"], \n    var_name=\"Case\", \n    value_name=\"$\"\n)\n\nchart = plot_balance_area_chart(\n    flat_discount_df, \n    title=\"(Real, Discounted at 6%): Comparison of Investing vs Coffee Drinking\"\n)\nchart.show()"
  },
  {
    "objectID": "posts/coffee/index.html#stochastic-model",
    "href": "posts/coffee/index.html#stochastic-model",
    "title": "How Much is a Coffee Worth to Me?",
    "section": "Stochastic Model",
    "text": "Stochastic Model\nBut you can’t just assume a 8.0% year on year return like that! Market performance isn’t guaranteed! protests Nick the dribbler, milk froth and steam spilling from his maw as if Smaug the dragon had recently left the Lonely Mountain, put on some weight and taken up residence in South Melbourne.\nAnd fair enough! From a risk perspective it’s worth examining what range of outcomes someone is exposed to when thinking about saving for the future. It’s also not as daunting a task as it might first appear, especially if we settle for a relatively simple to implement price model.\nFirst, we need data.\n\nHistorical Returns\nHere is the last 15 years of ASX200 market data:\n\n\nCode\nimport yfinance as yf\n\nprice_df = (\n    yf.Ticker(\"^AXJO\")\n    .history(period=\"15y\")\n    .reset_index()\n)\n\nchart = (\n    alt.Chart(price_df)\n    .mark_line(color=\"red\", strokeWidth=1)\n    .encode(\n        alt.X(\"Date:T\"),\n        alt.Y(\n            \"Close:Q\",\n            scale=alt.Scale(zero=False)\n        )\n    )\n    .properties(\n        title='ASX 200 Closing Price - 15 Years',\n        height=PLOT_HEIGHT,\n        width=PLOT_WIDTH,\n    )\n)\n\nchart.show()\n\n\n\n\n\n\n\n\nSince the model invests into “the market”, I’ll just take the ASX200 as representative of market movements to keep things simple.\nFor our forecast, we’ll need to transform this data into something a bit more convenient, the log returns, defined as:\n\\[\n\\begin{align}\n\\ln R_t = \\ln \\frac{P_t}{P_{t-1}}\n\\end{align}\n\\]\nWhere \\(P_t\\) is the closing price on a given day, and \\(P_{t-1}\\) is the closing price on the previous day.\n\n\nCode\nprice_df[\"Returns\"] = (\n    np.log(\n        price_df[\"Close\"] / price_df[\"Close\"].shift(1)\n    )\n)\n\nreturns_chart = (\n    alt.Chart(price_df)\n    .mark_line(color=\"limegreen\", strokeWidth=1)\n    .encode(\n        alt.X(\"Date:T\"),\n        alt.Y(\"Returns:Q\")\n    )\n    .properties(\n        title='ASX 200 Log Returns',\n        height=PLOT_HEIGHT,\n        width=PLOT_WIDTH,\n    )\n)\n\nreturns_chart.show()\n\n\n\n\n\n\n\n\nBecause we are (more or less) taking the derivative of price—the degree to which prices change day to day—we end up with a nice, relatively stationary signal of the market. Removing the time axis gives us a dataset that I can fit a distribution to:\n\n\nCode\nparams = stats.norm.fit(price_df[\"Returns\"].dropna())\ndist = stats.norm(*params)\nx_range = np.linspace(\n    price_df[\"Returns\"].min(), \n    price_df[\"Returns\"].max(),\n    250\n)\n\nchart = stat_plots.hist_dist_plot(\n    price_df,\n    col=\"Returns\",\n    scipy_dist=dist,\n    title=\"Normal Distribution Fitted to Returns\",\n    hist_color=\"limegreen\"\n)\n\nchart.show()\n\n\n\n\n\n\n\n\nThe normal distribution used above does a pretty poor job of capturing the data. A Student’s T distribution should better capture the tails:\n\n\nCode\nparams = stats.t.fit(price_df[\"Returns\"].dropna())\ndist = stats.t(*params)\nx_range = np.linspace(\n    price_df[\"Returns\"].min(), \n    price_df[\"Returns\"].max(),\n    250\n)\n\nchart = stat_plots.hist_dist_plot(\n    price_df,\n    col=\"Returns\",\n    scipy_dist=dist,\n    title=\"Student's T Distribution Fitted to Returns\",\n    hist_color=\"limegreen\"\n)\n\nchart.show()\n\n\n\n\n\n\n\n\nMuch better.\nAt this point it looks like we have a good-enough fit, but it is worth plotting samples back on a time-axis so we can directly compare what our model produces to actual historical returns:\n\n\nCode\nsample_df = pd.DataFrame({\n    \"Date\": price_df[\"Date\"],\n    \"Simulated Returns\": dist.rvs(size=len(price_df[\"Date\"]))\n})\n\nsample_chart = (\n    alt.Chart(sample_df)\n    .mark_line(color=\"orange\", strokeWidth=1)\n    .encode(\n        alt.X(\"Date:T\"),\n        alt.Y(\"Simulated Returns:Q\")\n    )\n)\n\ncombined_chart = (sample_chart + returns_chart).resolve_axis(x=\"shared\", y=\"shared\")\ncombined_chart.show()\n\n\n\n\n\n\n\n\n… and it does okay. The elephant in the room is COVID-19. Independent samples from a distribution can’t capture clustered periods of volatility like that. The samples are also a bit fuzzier in general—there is more volatility in the simulated market than was seen in reality. Overall I give the model a:\n\n\n\nThis could be my career catch phrase.\n\n\n\n\nGeometric Brownian Motion\nNow that we have a distribution, we can simulate forecasts from it.\nFirst, since the distribution is fitted to daily returns and our forecast will be monthly, an adjustment is needed:\n\n\nCode\ndef convert_t_dist_to_monthly(t_dist, days_per_month=21):\n    df, mu, sigma = t_dist.args[0], t_dist.args[1], t_dist.args[2]\n\n    # Monthly transformation\n    mu_monthly = days_per_month * mu\n    sigma_monthly = sigma * np.sqrt(days_per_month)\n\n    return stats.t(df=df, loc=mu_monthly, scale=sigma_monthly)\n\nmonthly_dist = convert_t_dist_to_monthly(dist)\n\n\nRight, now the fun stuff.\nIt turns out that you can cook up a stochastic forecast called a Geometric Brownian Motion (GBM) model quite easily with a numpy magic spell:\n\n\nCode\nsims = 1000\ninitial_balance = COFFEE_COST\n\ndef simulate_monthly_gbm(\n    monthly_returns_dist,\n    initial_balance: float,\n    num_months: int,\n    sims: int = 100\n) -&gt; np.ndarray:\n\n    return initial_balance * (\n        np.exp(\n            np.cumsum(\n                monthly_returns_dist.rvs(\n                    size=(num_months, sims)\n                ),\n                axis=0\n            )\n        )\n    )\n\nbalance = simulate_monthly_gbm(\n    monthly_dist,\n    initial_balance=COFFEE_COST,\n    num_months=num_months,\n    sims=sims\n)\n\n\nIn words, this is the exponentiated cumulative sum of log returns, or excumsumlogret.\n\n\n\n“EXCUMSUUUMMLOGREEEEET”\n\n\n\n\nCode\nchart = finance_plots.simulated_portfolio_plot(\n    simulated_balance=balance,\n    dates=dates,\n    title=\"Coffee Simulations\",\n    sims=100\n)\n\nchart.show()\n\n\n\n\n\n\n\n\nWhen you hold out your coffee cup and say the magic word, future worlds spout forth, in which the $6.0 purchase of that coffee is instead invested into the market.\nLooking at the end point of all those universes 20 years from now, we get a distribution of that coffee’s final value:\n\n\nCode\ndf = pd.DataFrame({\n    \"Sim\": np.arange(sims),\n    \"$\": balance[-1, :]\n})\n\nchart = (\n    alt.Chart(df)\n    .mark_bar()\n    .encode(\n        alt.X(\"$:Q\", bin=alt.Bin(maxbins=250)),\n        alt.Y(\n            \"count()\",\n            title=\"\",\n            axis=alt.Axis(labels=False, ticks=False, title=None),\n        )\n    )\n    .properties(\n        title='Final Coffee Value',\n        height=PLOT_HEIGHT,\n        width=PLOT_WIDTH,\n    )\n)\n\nchart.show()\n\n\n\n\n\n\n\n\n\n\nCode\npercentiles = [1, 10, 50, 90, 99]\n\ndf = pd.DataFrame(\n    {\n       \"Coffee Value\": np.percentile(df[\"$\"], q=percentiles),\n    }, \n    index=[f\"P{100 - p}\" for p in percentiles]\n).round()\n\ndf.T\n\n\n\n\n\n\n\n\n\nP99\nP90\nP50\nP10\nP1\n\n\n\n\nCoffee Value\n23.0\n45.0\n107.0\n249.0\n519.0\n\n\n\n\n\n\n\n\n\nSimulated Investments\nNow that we have a way of simulating forecasts, I can run the investing strategy (putting the price of a cup of coffee into the market each day) through the model to see how the portfolio might perform over the 20 year period.\n\n\nCode\ndef simulate_compounded_returns(\n    investments: np.ndarray,\n    monthly_returns_dist,\n    num_months: int,\n) -&gt; np.ndarray:\n\n    returns = np.exp(\n        monthly_returns_dist.rvs(size=(num_months, sims))\n    )\n\n    portfolio_values = np.zeros_like(returns)\n    for i in range(num_months):\n        # Initial investment\n        if i == 0:\n            portfolio_values[i] = investments[i] * returns[i]\n\n        # New investment + previously invested\n        else:\n            portfolio_values[i] = (\n                (portfolio_values[i-1] + investments[i]) * returns[i]\n            )\n\n    return portfolio_values\n\nportfolio = simulate_compounded_returns(\n    investments=investing,\n    monthly_returns_dist=monthly_dist,\n    num_months=num_months\n)\n\n\n\n\nCode\nchart = finance_plots.simulated_portfolio_plot(\n    portfolio,\n    dates,\n    title=\"Simulated Investment Portfolio\",\n    sims=100\n)\n\nchart.show()\n\n\n\n\n\n\n\n\n\n\nCode\ndf = pd.DataFrame({\n    \"Sim\": np.arange(sims),\n    \"$\": portfolio[-1, :]\n})\n\nchart = (\n    alt.Chart(df)\n    .mark_bar()\n    .encode(\n        alt.X(\"$:Q\", bin=alt.Bin(maxbins=250)),\n        alt.Y(\n            \"count()\",\n            title=\"\",\n            axis=alt.Axis(labels=False, ticks=False, title=None),\n        )\n    )\n    .properties(\n        title='Final Portfolio Value',\n        height=PLOT_HEIGHT,\n        width=PLOT_WIDTH,\n    )\n)\n\nchart.show()\n\n\n\n\n\n\n\n\nAnd if we discount at the conservative rate of 6%:\n\n\nCode\ndiscounted_portfolio = portfolio * discount_curve[:, np.newaxis]\n\nchart = finance_plots.simulated_portfolio_plot(\n    discounted_portfolio,\n    dates,\n    title=\"(Discounted) Simulated Investment Portfolio\",\n    sims=100\n)\nchart.show()\n\n\n\n\n\n\n\n\n\n\nCode\ndf = pd.DataFrame({\n    \"Sim\": np.arange(sims),\n    \"$\": discounted_portfolio[-1, :]\n})\n\nchart = (\n    alt.Chart(df)\n    .mark_bar()\n    .encode(\n        alt.X(\"$:Q\", bin=alt.Bin(maxbins=250)),\n        alt.Y(\n            \"count()\",\n            title=\"\",\n            axis=alt.Axis(labels=False, ticks=False, title=None),\n        )\n    )\n    .properties(\n        title='(Discounted) Final Portfolio Value',\n        height=PLOT_HEIGHT,\n        width=PLOT_WIDTH,\n    )\n)\n\nchart.show()\n\n\n\n\n\n\n\n\n\n\nCode\npercentiles = [1, 10, 50, 90, 99]\n\ndf = pd.DataFrame(\n    {\n       \"Nominal Returns\": np.percentile(portfolio[-1, :], q=percentiles),\n       \"Discounted Returns\": np.percentile(discounted_portfolio[-1, :], q=percentiles)\n    }, \n    index=[f\"P{100 - p}\" for p in percentiles]\n).round()\n\ndf.T\n\n\n\n\n\n\n\n\n\nP99\nP90\nP50\nP10\nP1\n\n\n\n\nNominal Returns\n82134.0\n134836.0\n268181.0\n502817.0\n950964.0\n\n\nDiscounted Returns\n24027.0\n39444.0\n78452.0\n147090.0\n278188.0\n\n\n\n\n\n\n\nSo we can’t know for sure how the market will perform over the next 20 years, but it looks like a good bet to drink less coffee and invest more. There’s quite a lot of positive skew in the model, so there is a lot of upside potential in investing that money.\n… That feels like a very underwhelming conclusion, after all that effort."
  },
  {
    "objectID": "posts/coffee/index.html#the-good",
    "href": "posts/coffee/index.html#the-good",
    "title": "How Much is a Coffee Worth to Me?",
    "section": "The Good",
    "text": "The Good\nEven though all models are wrong and every prediction is contingent on a set of assumptions and all that… I think the power of doing this is to stoke the imagination about what might be possible beyond the typical point estimate like 20.54% return over 10 years, especially when it comes to thinking about exposure—to risks as well as opportunities.\nHaving a single prediction doesn’t help you to think about the range of things that might happen, but a stochastic model like the simple forecast above does. Now you can think about how likely it is you’ll lose all your money, or how likely it is that curbing your coffee habit will make you a millionaire, or anything in between.\nAnd what about the implications of that range of outcomes? The lower tail predictions of the model might have extremely dire consequences that lead you not to act, for fear of the repercussions. The upper tail might be so attractive that it becomes obvious that the exposure is worth the entry price, even if the “expectation” doesn’t look that great. This kind of reasoning isn’t possible without uncertainty quantification.\nThe robustness of the model is always going to be a concern, but it seems to me a lot less flimsy than a single prediction. I’m not sure quantitative predictions about the future make much sense at all unless they quantify uncertainty somehow. Why draw a particular scenario out of a hat when there are an infinite set of others to choose from?"
  },
  {
    "objectID": "posts/coffee/index.html#the-bad",
    "href": "posts/coffee/index.html#the-bad",
    "title": "How Much is a Coffee Worth to Me?",
    "section": "The Bad",
    "text": "The Bad\nBut this also makes stochastic models way more dangerous—having a distribution as output might lead you or your colleagues to believe you’ve actually captured the possibilities objectively. You haven’t. What you have is the formalized consequences of your assumptions—in this case, a simplistic representation of past behaviour of the market propagated into the future—and that can still be useful, but it’s not the same as knowing what will actually happen. COVID19 is not in your model. World War III is not in your model.\nPoint estimates don’t carry that pretense. They’re dumb, but at least they’re honest about it. Or rather we are more honest about them.\nThis is a real tension I’ve felt in my career. The whole point of providing estimates is to help people make better decisions, but the uncertainty in those estimates is not true—it’s the output of a model. Models aren’t reality. This becomes obvious when you change a model hyperparameter and see all the estimates change—like an epistemology witch crawling out of the screen and whispering in your ear, it’s just a model, stupid. But that might not be as obvious when viewed from the outside.\n\n\n\n“But cross validation is so bori—AAAAAHHHH!”\n\n\nSo when I hand off my estimates… are we all just pretending that we “know” the uncertainty now? If the managers understand they should take the estimates with a “grain of salt”… how many grains of salt? At some point human intuition steps in and takes over again, which diminishes the point of the exercise."
  },
  {
    "objectID": "posts/coffee/index.html#the-coffee",
    "href": "posts/coffee/index.html#the-coffee",
    "title": "How Much is a Coffee Worth to Me?",
    "section": "The Coffee",
    "text": "The Coffee\nTake this coffee example. The model says that on paper Nick the investor is a lot better off, only having lost the value of the pleasure of a coffee… and the relationship building that goes along with it… and the chance conversations that could have really impacted his career… every single morning… for 20 years…\n\n\n\nMaybe the real investments were the friends we made along the way…\n\n\nAnd there it is! My intuitions (biased as they are) are wrestling with the implications of the model. I know it is not very sophisticated, I know that life is about more than maximizing financial gain, and I know that I can’t quantify those things, so it is inevitably neglecting many factors I care about.\nSo when faced with that doubt, my decision to drink coffee loops back to aesthetics, or how I want to live my life, not a quantitative, value maximizing rational one.\nSo modelling things is useful, but decision making is a lot more nuanced and human than numerical modelling and decision theory would like to make it.\n… I should probably cut down, though.\nThanks for reading."
  }
]