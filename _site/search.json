[
  {
    "objectID": "posts/copulas/index.html",
    "href": "posts/copulas/index.html",
    "title": "Correlated Sampling Made Easy",
    "section": "",
    "text": "A common “gotcha” with creating simple Monte Carlo scripts in Python or Excel is the request to add correlation to the distributions being sampled. This is a difficult thing to do in Excel without using plugins like AtRisk, but in Python, correlated sampling can be performed conveniently with the use of Copulas.\nThe fundamental intuition to build when working with copulas is that all continuous distributions can be remapped to a \\(\\text{Uniform}[0, 1]\\) distribution with their Cumulative Distribution Function (CDF), and transformed back with the inverse CDF, also called the Percent Point Function (PPF). Furthermore, sampling from any distribution can be done by running uniform samples through its PPF (that is how random variate sampling is done in general).\nWith that trick in mind, we can make use of the multivariate Gaussian distribution and its CDF to easily generate correlated samples of any distributions we want, which is very useful when creating numerical simulation models."
  },
  {
    "objectID": "posts/copulas/index.html#generate-correlated-samples",
    "href": "posts/copulas/index.html#generate-correlated-samples",
    "title": "Correlated Sampling Made Easy",
    "section": "Generate Correlated Samples",
    "text": "Generate Correlated Samples\nLet’s say we have three parameters \\(a\\), \\(b\\) and \\(c\\) that we want to sample with some correlation coefficient \\(\\rho\\). Before we worry about the specifics of those distributions, we can sample from a multivariate Gaussian with that correlation structure.\n\n\nCode\n# Number of samples\nn = 2500\n\n# Correlation matrix\nrho = 0.80\ncorr = np.array([\n    [1.0, rho, rho],\n    [rho, 1.0, rho],\n    [rho, rho, 1.0],\n])\n\n# Mean vector (zeros with standard mv normal)\nmu = [0.0, 0.0, 0.0]\n\n# Generate samples with dims (n, parameter)\nsamples = np.random.multivariate_normal(\n    mean=mu,\n    cov=corr,\n    size=n\n)\n\n\nThis generates 2500 samples from the standard multivariate Gaussian distribution with correlation \\(\\rho = 0.8\\).\n\n\nCode\ndf = pd.DataFrame({\n    \"a\": samples[:, 0],\n    \"b\": samples[:, 1],\n    \"c\": samples[:, 2],\n})\n\ndef create_pairplot(df: pd.DataFrame, color: str = \"dodgerblue\") -&gt; alt.RepeatChart:\n    return (\n        alt.Chart(df)\n        .mark_circle(\n            color=color,\n            opacity=0.25\n        )\n        .encode(\n            alt.X(\n                alt.repeat(\"column\"), \n                type=\"quantitative\", \n                scale=alt.Scale(zero=False)\n            ),\n            alt.Y(\n                alt.repeat(\"row\"), \n                type=\"quantitative\", \n                scale=alt.Scale(zero=False)\n            )\n        )\n        .properties(\n            height=PLOT_HEIGHT / 3,\n            width=180,\n        )\n        .repeat(\n            row=list(df.columns),\n            column=list(df.columns)\n        )\n    )\n\ncreate_pairplot(df, color=\"lightcoral\")\n\n\n\n\n\n\n\n\n\n\nCode\ndf.corr()\n\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\na\n1.000000\n0.795294\n0.796634\n\n\nb\n0.795294\n1.000000\n0.793701\n\n\nc\n0.796634\n0.793701\n1.000000"
  },
  {
    "objectID": "posts/copulas/index.html#transform-to-uniform",
    "href": "posts/copulas/index.html#transform-to-uniform",
    "title": "Correlated Sampling Made Easy",
    "section": "Transform to Uniform",
    "text": "Transform to Uniform\nWe can transform those samples to \\(\\text{Uniform} [0, 1]\\) by simply passing them through the standard normal CDF.\n\n\nCode\nuniform_samples = stats.norm.cdf(df.values)\nuniform_df = pd.DataFrame({\n    \"a\": uniform_samples[:, 0],\n    \"b\": uniform_samples[:, 1],\n    \"c\": uniform_samples[:, 2],\n})\n\ncreate_pairplot(uniform_df, color=\"pink\")\n\n\n\n\n\n\n\n\nNow the sample are transformed to the uniform domain, but importantly their correlation structure has been preserved.\n\n\nCode\nuniform_df.corr()\n\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\na\n1.000000\n0.779274\n0.784258\n\n\nb\n0.779274\n1.000000\n0.770140\n\n\nc\n0.784258\n0.770140\n1.000000"
  },
  {
    "objectID": "posts/copulas/index.html#transform-to-distributions",
    "href": "posts/copulas/index.html#transform-to-distributions",
    "title": "Correlated Sampling Made Easy",
    "section": "Transform to Distributions",
    "text": "Transform to Distributions\nNow we have a bunch of uniformly distributed random samples that have our desired correlation structure. All that is left is to map the samples to our desired distributions using their respective PPFs.\nLet’s define our distributions as:\n\\[\n\\begin{align}\nA &\\sim \\text{Normal}(500, 50) \\\\\nB &\\sim \\text{Gamma}(2, 5) \\\\\nC &\\sim \\text{Beta}(5, 8)\n\\end{align}\n\\]\nNotice I’m not using normal distribtions exclusively, I can use the copula to map to whatever distributions I want. Note only that that the more skewed and kurtotic the distributions, the more warping will occur in their correlations out the other end. There are other types of Copula that can handle this better than the Gaussian Copula used here.\n\n\nCode\ndist_df = pd.DataFrame({\n    \"a\": stats.norm(500, 25).ppf(uniform_df[\"a\"]),\n    \"b\": stats.gamma(a=2, scale=5, loc=0).ppf(uniform_df[\"b\"]),\n    \"c\": stats.beta(5, 8).ppf(uniform_df[\"c\"])\n})\n\ncreate_pairplot(dist_df, color=\"orange\")\n\n\n\n\n\n\n\n\n\n\nCode\ndist_df.corr()\n\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\na\n1.000000\n0.754574\n0.794633\n\n\nb\n0.754574\n1.000000\n0.758680\n\n\nc\n0.794633\n0.758680\n1.000000"
  },
  {
    "objectID": "posts/copulas/index.html#comparison-with-uncorrelated-sampling",
    "href": "posts/copulas/index.html#comparison-with-uncorrelated-sampling",
    "title": "Correlated Sampling Made Easy",
    "section": "Comparison with Uncorrelated Sampling",
    "text": "Comparison with Uncorrelated Sampling\nIf we skip the Copula and just sample from our distributions, we get this:\n\n\nCode\nunc_dist_df = pd.DataFrame({\n    \"a\": stats.norm(500, 25).rvs(size=n),\n    \"b\": stats.gamma(a=2, scale=5, loc=0).rvs(size=n),\n    \"c\": stats.beta(5, 8).rvs(size=n)\n})\n\ncreate_pairplot(unc_dist_df, color=\"lightblue\")\n\n\n\n\n\n\n\n\n\n\nCode\nunc_dist_df.corr()\n\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\na\n1.000000\n-0.011560\n-0.008544\n\n\nb\n-0.011560\n1.000000\n0.026902\n\n\nc\n-0.008544\n0.026902\n1.000000\n\n\n\n\n\n\n\n\nEffect on Monte Carlo Simulation\nLet’s say that whatever these parameters represent, we want to know the result of this expression:\n\\[\ny = ab^c\n\\]\n\n\nCode\nunc_dist_df[\"y\"] = unc_dist_df[\"a\"] * unc_dist_df[\"b\"] ** unc_dist_df[\"c\"]\ndist_df[\"y\"] = dist_df[\"a\"] * dist_df[\"b\"] ** dist_df[\"c\"]\n\nunc_dist_df[\"Case\"] = \"Uncorrelated\"\ndist_df[\"Case\"] = \"Correlated\"\n\ncombined_df = pd.concat([dist_df, unc_dist_df], ignore_index=True)\n\n# Filter tail\nupper_lim = np.percentile(combined_df[\"y\"], q=99.5)\ncombined_df = combined_df[\n    combined_df[\"y\"] &lt; upper_lim\n]\n\nchart = (\n    alt.Chart(combined_df)\n    .mark_bar(opacity=0.75)\n    .encode(\n        x=alt.X(\"y:Q\", bin=alt.Bin(maxbins=50), axis=alt.Axis(title=\"y\")),\n        y=alt.Y(\n            \"count():Q\", \n            axis=alt.Axis(labels=False, ticks=False, title=None),\n            stack=False\n        ),\n        color=alt.Color(\n            \"Case:N\", \n            scale=alt.Scale(\n                domain=[\"Correlated\", \"Uncorrelated\"], \n                range=[\"orange\", \"lightblue\"]\n            ),\n            legend=alt.Legend(orient=\"top-right\", title=\"\")\n        )\n    )\n    .properties(height=PLOT_HEIGHT, width=PLOT_WIDTH)\n)\n\n\nchart.show()\n\n\n\n\n\n\n\n\n\n\nCode\npercentiles = [10, 50, 90]\n\nresult_df = pd.DataFrame({\n    \"10th\": np.round(np.percentile(dist_df[\"y\"], q=percentiles), 3),\n    \"50th\": np.round(np.percentile(unc_dist_df[\"y\"], q=percentiles), 3)\n}).T\n\n# Fix column labels\nresult_df.columns = [f\"P{p}\" for p in percentiles]\nresult_df.index = [\"Correlated\", \"Uncorrelated\"]\n\nresult_df\n\n\n\n\n\n\n\n\n\nP10\nP50\nP90\n\n\n\n\nCorrelated\n596.300\n1109.349\n2667.278\n\n\nUncorrelated\n659.867\n1047.592\n1916.006\n\n\n\n\n\n\n\nThe correlated samples show a wider uncertainty range in the result than the uncorrelated samples, as is expected. This may be an important detail to capture depending on the sensitivity of the analysis. The effect of correlation can also be quite unintuitive, so it is always worth checking the effect it has on results."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Interpreting Depositional Environments with Bayes\n\n\nModelling depositional environments with a Stepped Dirichlet Process\n\n\n\ncode\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJun 3, 2025\n\n\nNicholas Dorsch\n\n\n\n\n\n\n\n\n\n\n\n\nAmy, Brett and the Cost of Living Crisis\n\n\nA Zero-Inflated Model of Household Spending\n\n\n\ncode\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJun 2, 2025\n\n\nNicholas Dorsch\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelated Outcomes\n\n\n\n\n\n\ncode\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJun 1, 2025\n\n\nNicholas Dorsch\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelated Sampling Made Easy\n\n\n\n\n\n\ncode\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nMay 31, 2025\n\n\nNicholas Dorsch\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is this blog for?\n\n\n\n\n\n\nupdates\n\n\n\n\n\n\n\n\n\nMay 28, 2025\n\n\nNicholas Dorsch\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Nicholas Dorsch - Geoscientist/Data Analyist\nI come from a geology and geophysics background working in the oil and gas industry, but over the years have developed a passion for bayesian statistics and data science more broadly. My skillset includes a mixture of geology and geophysics workflows as well as more general purpose statistical modelling. The intersection of these things has allowed me to work on fairly unique projects during my career.\nI strongly believe that bayesian statistical models offer the most robust and flexible framework for uncertainty quantification, as well as decision making under uncertainty. For that reason they have become my career focus.\nMore recently I have developed more data engineering skills, working with database design and management. My goal is to become a “full stack” data scientist, capable of managing and deploying analytical processes end to end, to help organisations make the best use of their data.\n\n\nContact information\nbased in: Melbourne, Australia\navailability: currently a full time employee of Karoon Energy\nemail: nickjdorsch@gmail.com"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "What is this blog for?",
    "section": "",
    "text": "I’m making this blog to better organize my thoughts, my work portfolio and to develop my writing skills, mostly through technical sharing. It will probably contain writing on statistics, programming and the occasional book review."
  },
  {
    "objectID": "posts/outcomes/index.html",
    "href": "posts/outcomes/index.html",
    "title": "Correlated Outcomes",
    "section": "",
    "text": "In the previous post I explained how Copulas can be used to sample distributions with correlation. A nice extention of that logic is to simulate discrete outcomes with correlation as well.\n\n\nThe modelling of independent and identically distributed trials is easily done with the Binomial distribution. For example, we can simulate tossing a coin 50 times like this:\n\n\nCode\n# Simulate 5000 experiments, tossing a coin 50 times\ntosses = 50\nn = 5000\n\n# Assume a fair coin\np = 0.5\n\nheads = np.random.binomial(tosses, p, n)\n\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nx_domain = [0, tosses]\ndef plot_disrete_histogram(df, col: str, color: str = \"dodgerblue\") -&gt; alt.Chart:\n    tick_values = list(range(0, tosses + 1, tosses // 10))\n\n    return (\n        alt.Chart(df)\n        .mark_bar(color=color, opacity=0.75)\n        .encode(\n            alt.X(\n                f\"{col}:Q\", \n                bin=alt.Bin(step=1), \n                scale=alt.Scale(domain=[0, tosses]),\n                axis=alt.Axis(values=tick_values, format=\".0f\")\n            ),\n            alt.Y(\"count():Q\")\n        )\n        .properties(\n            height=PLOT_HEIGHT,\n            width=PLOT_WIDTH\n        )\n    )\n\nplot_disrete_histogram(df,\"Heads\").show()\n\n\n\n\n\n\n\n\nThe above plot shows the number of heads thrown out of 50 total tosses, repeated 5000 times.\nThis can be made a bit more flexible by generating our own random uniform samples instead of directly using numpy’s random.binomial function:\n\n\nCode\n# Simulate 2D array of 0-1 values with dims (tosses, n)\nuniform_simdata = np.random.uniform(0, 1, size=(tosses, n))\n\n# Convert to a boolean array based on the p value\noutcomes = np.where(\n    uniform_simdata &lt; p,\n    True,\n    False\n)\n\n# Sum along the tosses axis to get total heads per experiment\nheads = np.sum(outcomes, axis=0)\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nplot_disrete_histogram(df, \"Heads\", color=\"coral\").show()\n\n\n\n\n\n\n\n\nThis might seem a bit inconvenient, but when simulating correlated events it becomes an effective way of getting the desired result.\n\n\n\nBecause we are now using uniform samples to create the outcomes, we can just feed correlated uniform samples into the process instead. This will correlate the outcomes.\nHere is a correlation of \\(0.25\\) shared across all coin tosses:\n\n\nCode\ndef create_simple_correlation_matrix(num: int, corr: float) -&gt; np.ndarray:\n    matrix = np.full((num, num), corr)\n    np.fill_diagonal(matrix, 1)\n    return matrix\n\ncorr = create_simple_correlation_matrix(num=tosses, corr=0.25)\n\n# Use a copula to generate correlated uniform samples\nsamples = stats.norm.cdf(\n    np.random.multivariate_normal(\n        mean=np.zeros(tosses),\n        cov=corr,\n        size=n\n    )\n).T\n\noutcomes = np.where(\n    samples &lt; p,\n    True,\n    False\n)\n\nheads = np.sum(outcomes, axis=0)\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nplot_disrete_histogram(df, \"Heads\", color=\"orange\").show()\n\n\n\n\n\n\n\n\nAnd another with a correlation of 0.75:\n\n\nCode\ncorr = create_simple_correlation_matrix(num=tosses, corr=0.75)\n\n# Use a copula to generate correlated uniform samples\nsamples = stats.norm.cdf(\n    np.random.multivariate_normal(\n        mean=np.zeros(tosses),\n        cov=corr,\n        size=n\n    )\n).T\n\noutcomes = np.where(\n    samples &lt; p,\n    True,\n    False\n)\n\nheads = np.sum(outcomes, axis=0)\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nplot_disrete_histogram(df, \"Heads\", color=\"yellow\").show()\n\n\n\n\n\n\n\n\nThe correlation between events has the effect of bifurcating the results, because events are firing together more often. A correlation of \\(1\\) using this method would return 0 heads or 100 heads, with nothing in between.\nThese examples are using a simple correlation matrix filled with the same value, but richer correlation structures can be modelled using covariance functions. The correlations might vary through time or space (or both), which means trials closer together may be more tightly related, for example. This would lead to sampling from a “binomial process” rather than the regular binomial distribution.\n\n\n\nThe framework outlined in this post and the previous provides a toolkit for numerically simulating many 0-dimensional problems (one where the answer is one number, not a higher dimensional array like a 2D map or 3D grid). Because these problems in simulation return a 1D array of results, I often refer to them as 1D models, but this can be a confusing term as the input space may have many dimensions – 1D refers only to the output space.\nMany real world problems boil down to what are called Zero-inflated models, where an event occurs with some probability \\(p\\), returning a random variable, or else returns zero:\n\\[\nX \\sim\n\\begin{cases}\n0, & \\text{with probability } 1 - p \\\\\nY \\sim \\mathcal{D}(\\theta), & \\text{with probability } p\n\\end{cases}\n\\]\nI’ll make things a bit more concrete with an example in my next post."
  },
  {
    "objectID": "posts/outcomes/index.html#independent-events",
    "href": "posts/outcomes/index.html#independent-events",
    "title": "Correlated Outcomes",
    "section": "",
    "text": "The modelling of independent and identically distributed trials is easily done with the Binomial distribution. For example, we can simulate tossing a coin 50 times like this:\n\n\nCode\n# Simulate 5000 experiments, tossing a coin 50 times\ntosses = 50\nn = 5000\n\n# Assume a fair coin\np = 0.5\n\nheads = np.random.binomial(tosses, p, n)\n\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nx_domain = [0, tosses]\ndef plot_disrete_histogram(df, col: str, color: str = \"dodgerblue\") -&gt; alt.Chart:\n    tick_values = list(range(0, tosses + 1, tosses // 10))\n\n    return (\n        alt.Chart(df)\n        .mark_bar(color=color, opacity=0.75)\n        .encode(\n            alt.X(\n                f\"{col}:Q\", \n                bin=alt.Bin(step=1), \n                scale=alt.Scale(domain=[0, tosses]),\n                axis=alt.Axis(values=tick_values, format=\".0f\")\n            ),\n            alt.Y(\"count():Q\")\n        )\n        .properties(\n            height=PLOT_HEIGHT,\n            width=PLOT_WIDTH\n        )\n    )\n\nplot_disrete_histogram(df,\"Heads\").show()\n\n\n\n\n\n\n\n\nThe above plot shows the number of heads thrown out of 50 total tosses, repeated 5000 times.\nThis can be made a bit more flexible by generating our own random uniform samples instead of directly using numpy’s random.binomial function:\n\n\nCode\n# Simulate 2D array of 0-1 values with dims (tosses, n)\nuniform_simdata = np.random.uniform(0, 1, size=(tosses, n))\n\n# Convert to a boolean array based on the p value\noutcomes = np.where(\n    uniform_simdata &lt; p,\n    True,\n    False\n)\n\n# Sum along the tosses axis to get total heads per experiment\nheads = np.sum(outcomes, axis=0)\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nplot_disrete_histogram(df, \"Heads\", color=\"coral\").show()\n\n\n\n\n\n\n\n\nThis might seem a bit inconvenient, but when simulating correlated events it becomes an effective way of getting the desired result."
  },
  {
    "objectID": "posts/outcomes/index.html#correlated-events",
    "href": "posts/outcomes/index.html#correlated-events",
    "title": "Correlated Outcomes",
    "section": "",
    "text": "Because we are now using uniform samples to create the outcomes, we can just feed correlated uniform samples into the process instead. This will correlate the outcomes.\nHere is a correlation of \\(0.25\\) shared across all coin tosses:\n\n\nCode\ndef create_simple_correlation_matrix(num: int, corr: float) -&gt; np.ndarray:\n    matrix = np.full((num, num), corr)\n    np.fill_diagonal(matrix, 1)\n    return matrix\n\ncorr = create_simple_correlation_matrix(num=tosses, corr=0.25)\n\n# Use a copula to generate correlated uniform samples\nsamples = stats.norm.cdf(\n    np.random.multivariate_normal(\n        mean=np.zeros(tosses),\n        cov=corr,\n        size=n\n    )\n).T\n\noutcomes = np.where(\n    samples &lt; p,\n    True,\n    False\n)\n\nheads = np.sum(outcomes, axis=0)\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nplot_disrete_histogram(df, \"Heads\", color=\"orange\").show()\n\n\n\n\n\n\n\n\nAnd another with a correlation of 0.75:\n\n\nCode\ncorr = create_simple_correlation_matrix(num=tosses, corr=0.75)\n\n# Use a copula to generate correlated uniform samples\nsamples = stats.norm.cdf(\n    np.random.multivariate_normal(\n        mean=np.zeros(tosses),\n        cov=corr,\n        size=n\n    )\n).T\n\noutcomes = np.where(\n    samples &lt; p,\n    True,\n    False\n)\n\nheads = np.sum(outcomes, axis=0)\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nplot_disrete_histogram(df, \"Heads\", color=\"yellow\").show()\n\n\n\n\n\n\n\n\nThe correlation between events has the effect of bifurcating the results, because events are firing together more often. A correlation of \\(1\\) using this method would return 0 heads or 100 heads, with nothing in between.\nThese examples are using a simple correlation matrix filled with the same value, but richer correlation structures can be modelled using covariance functions. The correlations might vary through time or space (or both), which means trials closer together may be more tightly related, for example. This would lead to sampling from a “binomial process” rather than the regular binomial distribution."
  },
  {
    "objectID": "posts/outcomes/index.html#bringing-things-together",
    "href": "posts/outcomes/index.html#bringing-things-together",
    "title": "Correlated Outcomes",
    "section": "",
    "text": "The framework outlined in this post and the previous provides a toolkit for numerically simulating many 0-dimensional problems (one where the answer is one number, not a higher dimensional array like a 2D map or 3D grid). Because these problems in simulation return a 1D array of results, I often refer to them as 1D models, but this can be a confusing term as the input space may have many dimensions – 1D refers only to the output space.\nMany real world problems boil down to what are called Zero-inflated models, where an event occurs with some probability \\(p\\), returning a random variable, or else returns zero:\n\\[\nX \\sim\n\\begin{cases}\n0, & \\text{with probability } 1 - p \\\\\nY \\sim \\mathcal{D}(\\theta), & \\text{with probability } p\n\\end{cases}\n\\]\nI’ll make things a bit more concrete with an example in my next post."
  },
  {
    "objectID": "posts/copulas/index.html#summary-of-process",
    "href": "posts/copulas/index.html#summary-of-process",
    "title": "Correlated Sampling Made Easy",
    "section": "Summary of Process",
    "text": "Summary of Process\n\n\n\n\n\ngraph TD\n    corr[\"Correlation Matrix\"]\n    mvn[\"Standard MvNorm(0, corr)\"]\n\n    corr --&gt; mvn --&gt; mvn_samples\n\n    mvn_samples[\"MvNorm Samples [2500, 3]\"]\n    uniform[\"Uniform Samples [2500, 3]\"]\n\n    mvn_samples --&gt;|Normal CDF| uniform\n\n    dist_a[\"a\"]\n    dist_b[\"b\"]\n    dist_c[\"c\"]\n\n    dist_a_samples[\"a Samples [2500]\"]\n    dist_b_samples[\"b Samples [2500]\"]\n    dist_c_samples[\"c Samples [2500]\"]\n\n    dist_a ---&gt;|PPF| dist_a_samples\n    dist_b ---&gt;|PPF| dist_b_samples\n    dist_c ---&gt;|PPF| dist_c_samples\n\n    uniform --&gt; dist_a_samples\n    uniform --&gt; dist_b_samples\n    uniform --&gt; dist_c_samples"
  },
  {
    "objectID": "posts/spending/index.html",
    "href": "posts/spending/index.html",
    "title": "Amy, Brett and the Cost of Living Crisis",
    "section": "",
    "text": "To make the processes outlined in the previous two posts more concrete, the following is the story of a young couple trying to manage a household budget. I’ll use an aggregated zero-inflated model of spending to simulate monthly expenses."
  },
  {
    "objectID": "posts/spending/index.html#audrey",
    "href": "posts/spending/index.html#audrey",
    "title": "Audrey, Brett and the Cost of Living Crisis",
    "section": "Audrey",
    "text": "Audrey\n\nAudrey is a freelancer and goes to work on 85% of days, where she spends no money\nOn days she is not working, she spends anywhere between $20 and $150\n\n\\[\nA \\sim\n\\begin{cases}\n0, & \\text{with probability } 0.15 \\\\\nC_A \\sim \\text{PERT}(20, 50, 150), & \\text{with probability } 0.85\n\\end{cases}\n\\]"
  },
  {
    "objectID": "posts/spending/index.html#brett",
    "href": "posts/spending/index.html#brett",
    "title": "Amy, Brett and the Cost of Living Crisis",
    "section": "Brett",
    "text": "Brett\n\nBrett is unemployed, and on a given day has a 70% chance of spending\nWhen he spends, usually gambling, he makes a real impact, spending anywhere between $20 on a cheeky bet to $1500 when “we’re on here…”\nOn days Amy is home, Brett tends not to gamble, but will still place bets while on the loo, out of Amy’s sight\nThe amount Brett bets depends on Amy’s spending… if she is leaving money on the table, he tends to use it, but if she’s making big purchases he keeps his bets smaller.\n\n\\[\nB \\sim\n\\begin{cases}\n0, & \\text{with probability } 0.3 \\\\\nC_B \\sim \\text{PERT}(20, 20, 1500), & \\text{with probability } 0.7\n\\end{cases}\n\\]\nThe cost of household essentials is \\(\\$7500\\) a month. So the total monthly expenses \\(C_{\\text{total}}\\) can be given by:\n\\[\nC_{\\text{total}} = 7500 + (30(A + B))\n\\]"
  },
  {
    "objectID": "posts/spending/index.html#spending-days",
    "href": "posts/spending/index.html#spending-days",
    "title": "Amy, Brett and the Cost of Living Crisis",
    "section": "Spending Days",
    "text": "Spending Days\nFirst I’ll simulate whether or not Amy and Brett spend money on a given day, and by extension their spending days in each month:\n\n\nCode\n# Simulate a month of spending days (True or False) for Amy and Brett\ndays = 30\nn = 2500\n\np_a = 0.15\np_b = 0.7\n\n# Negative correlation between Amy and Brett spending days\nrho = -0.95\nspend_corr_matrix = np.array([\n    [1.0, rho],\n    [rho, 1.0],\n])\n\n# Use a Copula to generate correlated spending days for the month\nuniform_simdata = stats.norm.cdf(\n    np.random.multivariate_normal(\n        mean=[0, 0],\n        cov=spend_corr_matrix,\n        size=(n, days)\n    )\n)\n\n# Map to binary outcomes (True = spend, False = no spend)\nspending_days = np.where(\n    uniform_simdata &lt; [p_a, p_b],\n    True,\n    False\n)\n\ndf = pd.DataFrame({\n    \"Amy\": np.sum(spending_days[:, :, 0], axis=1),\n    \"Brett\": np.sum(spending_days[:, :, 1], axis=1),\n})\nflat_df = df.melt(var_name=\"Person\", value_name=\"Spending Days\")\n\ndef plot_disrete_histogram(df, col: str, color: str = \"dodgerblue\") -&gt; alt.Chart:\n    tick_values = list(range(0, days + 1))\n    return (\n        alt.Chart(df)\n        .mark_bar(color=color, opacity=0.5)\n        .encode(\n            alt.X(\n                f\"{col}:Q\", \n                bin=alt.Bin(step=1), \n                scale=alt.Scale(domain=[0, days]),\n                axis=alt.Axis(values=tick_values, format=\".0f\"),\n            ),\n            alt.Y(\n                \"count():Q\", \n                stack=None,\n                title='',\n                axis=alt.Axis(\n                    labels=False, \n                    ticks=False, \n                    grid=False, \n                    domain=False\n                )\n            ),\n            alt.Color(\n                \"Person:N\",\n                legend=alt.Legend(orient=\"top-right\", title=\"\")\n            ),\n        )\n        .properties(\n            title=\"Number of Days Money Spent in Month\",\n            height=PLOT_HEIGHT,\n            width=PLOT_WIDTH\n        )\n    )\n\nplot_disrete_histogram(flat_df, \"Spending Days\").show()\n\n\n\n\n\n\n\n\nSince Amy is often at work, she spends money on less days during the month. Brett, on the other hand is at home, in his element, making legendary bets.\nIt’s not obvious from the plot above, but Brett is less likely to gamble on days that Amy is at home. A 2D plot can show this better:\n\n\nCode\ntick_values = list(range(0, days + 1, 2))\nchart = (\n    alt.Chart(df)\n    .mark_rect()\n    .encode(\n        alt.X(\n            \"Amy:Q\", \n            bin=alt.Bin(maxbins=days // 2),\n            scale=alt.Scale(domain=[0, days]),\n            axis=alt.Axis(values=tick_values)\n        ),\n        alt.Y(\n            \"Brett:Q\", \n            bin=alt.Bin(maxbins=days // 2),\n            scale=alt.Scale(domain=[0, days]),\n            axis=alt.Axis(values=tick_values)\n        ),\n        alt.Color(\n            \"count():Q\",\n            scale=alt.Scale(scheme=\"oranges\"),\n            legend=None\n\n        )\n    )\n    .properties(\n        title=\"Amy vs Brett Spending Days in Month\",\n        height=PLOT_HEIGHT,\n        width=PLOT_WIDTH\n    )\n)\n\nchart.show()\n\n\n\n\n\n\n\n\nThere is a negative trend between Amy’s days at home and Brett’s spending. This is because on months where Amy isn’t getting as much work, she is at home and Brett has less opportunities to gamble. However, on months where she is working a lot, Brett is at home alone, turning the place into his personal casino."
  },
  {
    "objectID": "posts/spending/index.html#amount-spent",
    "href": "posts/spending/index.html#amount-spent",
    "title": "Amy, Brett and the Cost of Living Crisis",
    "section": "Amount Spent",
    "text": "Amount Spent\nNext, I’ll simulate how much Amy and Brett each spend on a given day. I’ll use the PERT distribution, as it is a convenient and flexible choice for subjective models like this.\n\n\nCode\nclass PERT:\n    def __init__(self, minimum: float, mode: float, maximum: float):\n        self.minimum = minimum\n        self.mode = mode\n        self.maximum = maximum\n\n        self.alpha = (\n            (4 * (self.mode - self.minimum) / (self.maximum - self.minimum)) + 1\n        )\n        self.beta = (\n            (4 * (self.maximum - self.mode) / (self.maximum - self.minimum)) + 1\n        )\n\n        # Beta distribution on [0,1]\n        self.beta_dist = stats.beta(self.alpha, self.beta)\n\n    def pdf(self, x):\n        \"\"\"\n        Calculate the PDF of the PERT distribution at x.\n        \"\"\"\n        if x &lt; self.minimum or x &gt; self.maximum:\n            return 0.0\n\n        # Scale x to [0,1]\n        x_scaled = (x - self.minimum) / (self.maximum - self.minimum)\n        return self.beta_dist.pdf(x_scaled) / (self.maximum - self.minimum)\n\n    def rvs(self, shape: int | tuple[int] = 1) -&gt; np.ndarray:\n        \"\"\"\n        Generate random variates of given size from the PERT distribution.\n        \"\"\"\n        samples_scaled = self.beta_dist.rvs(size=shape)\n        # Scale samples back to [minimum, maximum]\n        return self.minimum + samples_scaled * (self.maximum - self.minimum)\n\n    def ppf(self, q):\n        \"\"\"\n        Percent-point function (inverse CDF) at q of the PERT distribution.\n        \"\"\"\n        x_scaled = self.beta_dist.ppf(q)\n        return self.minimum + x_scaled * (self.maximum - self.minimum)\n\n    def plot(\n        self, \n        color: str = \"dodgerblue\", \n        title: str = \"PERT PDF\",\n        height=PLOT_HEIGHT, \n        width=PLOT_WIDTH\n    ) -&gt; alt.Chart:\n        x_vals = np.linspace(self.minimum, self.maximum, 200)\n        pdf_vals = [self.pdf(x) for x in x_vals]\n\n        df = pd.DataFrame({\n            'x': x_vals,\n            'pdf': pdf_vals\n        })\n\n        chart = (\n            alt.Chart(df)\n            .mark_area(line={\"color\": color}, color=color, opacity=0.5)\n            .encode(\n                x=alt.X('x', title='$'),\n                y=alt.Y(\n                    'pdf', \n                    title='',\n                    axis=alt.Axis(\n                        labels=False, \n                        ticks=False, \n                        grid=False, \n                        domain=False\n                    )\n                )\n            )\n            .properties(\n                width=width,\n                height=height,\n                title=title\n            )\n        )\n        return chart\n\n\n\nAmy\n\\[\nC_A \\sim \\text{PERT}(20, 50, 150)\n\\]\nAmy is not a big spender. She is usually at work, but when she has a day off she never spends more than $150 during the day, and usually hovers around the $50 mark.\n\n\nCode\nspend_a = PERT(20, 50, 150)\nspend_a.plot(title=\"Amy's Daily Spending\").show()\n\n\n\n\n\n\n\n\n\n\nBrett\n\\[\nC_B \\sim \\text{PERT}(20, 20, 1500)\n\\]\nBrett’s habits are a lot more volatile. He is always at home, and overall much more likely to spend. When he does spend, it is anything between a small $20 bet to significant $1500 wagers.\n\n\nCode\nspend_b = PERT(20, 20, 1500)\nspend_b.plot(title=\"Brett's Daily Spending\", color=\"orange\").show()"
  },
  {
    "objectID": "posts/spending/index.html#month-of-spending",
    "href": "posts/spending/index.html#month-of-spending",
    "title": "Amy, Brett and the Cost of Living Crisis",
    "section": "Month of Spending",
    "text": "Month of Spending\nUsing the spending day simulation in combination with correlated sampling of the two spending distributions, I can estimate the monthly spending of each member of the household:\n\n\nCode\n# Negative correlation between Amy and Brett spending\nrho = -0.95\nspend_corr_matrix = np.array([\n    [1.0, rho],\n    [rho, 1.0],\n])\n\n# Use a Copula to generate correlated spending days for the month\nuniform_simdata = stats.norm.cdf(\n    np.random.multivariate_normal(\n        mean=[0, 0],\n        cov=spend_corr_matrix,\n        size=(n, days)\n    )\n)\n\nsimdata_a = spend_a.ppf(uniform_simdata[:, :, 0])\nsimdata_b = spend_b.ppf(uniform_simdata[:, :, 1])\n\n# Mask with array of spending days\nsimdata_a = np.where(\n    spending_days[:, :, 0],\n    simdata_a,\n    0\n)\nsimdata_b = np.where(\n    spending_days[:, :, 1],\n    simdata_b,\n    0\n)\n\ndf = pd.DataFrame({\n    \"Amy\": np.sum(simdata_a, axis=1),\n    \"Brett\": np.sum(simdata_b, axis=1),\n})\n\nflat_df = df.melt(var_name=\"Person\", value_name=\"Spending\")\ndf[\"TOTAL\"] = df[\"Amy\"] + df[\"Brett\"]\n\n\nPlotting to show Amy and Brett’s spending separately paints a pretty clear picture:\n\n\nCode\nchart = (\n    alt.Chart(flat_df)\n    .mark_bar(opacity=0.5)\n    .encode(\n        alt.X(\n            \"Spending:Q\", \n            bin=alt.Bin(maxbins=100)\n        ),\n        alt.Y(\"count():Q\", stack=None),\n        alt.Color(\n            \"Person:N\",\n            legend=alt.Legend(orient=\"top-right\", title=\"\")\n        )\n    )\n    .properties(\n        width=PLOT_WIDTH,\n        height=PLOT_HEIGHT,\n        title=\"Household Monthly Spending\"\n    )\n)\n\nchart.show()\n\n\n\n\n\n\n\n\nBrett has a problem.\nThe negative correlation between Amy and Brett’s spending is in the model, though it doesn’t show up very strongly in the aggregated monthly spending:\n\n\nCode\nchart = (\n    alt.Chart(df)\n    .mark_point()\n    .encode(\n        alt.X(\"Amy:Q\"),\n        alt.Y(\"Brett:Q\"),\n        alt.Color(\n            \"TOTAL:Q\",\n            legend=alt.Legend(orient=\"top-right\")\n        )\n    )\n    .properties(\n        width=PLOT_WIDTH,\n        height=PLOT_HEIGHT,\n        title=\"Amy vs Brett Monthly Spending\"\n    )\n)\n\nchart.show()"
  },
  {
    "objectID": "posts/spending/index.html#overall-spend",
    "href": "posts/spending/index.html#overall-spend",
    "title": "Audrey, Brett and the Cost of Living Crisis",
    "section": "Overall Spend",
    "text": "Overall Spend\nNow I can plug the above simulated numbers into the formula:\n\\[\nC_{\\text{total}} = 6000 + (30(A + B))\n\\]\nto arrive at the final monthly total.\nAssuming Audrey makes $500 per day worked after tax, I can also compute a monthly net income.\n\n\nCode\ndef calculate_total_monthly_expenses(\n    audrey_spend, \n    brett_spend, \n    base_expense: float = 6000\n) -&gt; np.ndarray:\n    audrey_total_spend = np.sum(audrey_spend, axis=1)\n    brett_total_spend = np.sum(brett_spend, axis=1)\n\n    return base_expense + audrey_total_spend + brett_total_spend\n\ntotal_expenses = calculate_total_monthly_expenses(\n    audrey_spend=simdata_a,\n    brett_spend=simdata_b,\n    base_expense=6000\n)\n\nincome_per_day = 500\nincome = np.sum(\n    np.where(\n        ~spending_days[:, :, 0],\n        income_per_day, \n        0\n    ),\n    axis=1\n)\n\ndf = pd.DataFrame({\n    \"Expenses\": total_expenses,\n    \"Income\": income,\n    \"Net Income\": income - total_expenses\n})\nflat_df = df.melt(var_name=\"Total\", value_name=\"Amount\")\nflat_df = flat_df.sample(5000)\n\nchart = (\n    alt.Chart(flat_df)\n    .mark_bar(opacity=0.5)\n    .encode(\n        alt.X(\"Amount:Q\", bin=alt.Bin(maxbins=50)),\n        alt.Y(\n            \"count():Q\", \n            stack=None,\n            title='',\n            axis=alt.Axis(\n                labels=False, \n                ticks=False, \n                grid=False, \n                domain=False\n            ),\n        ),\n        alt.Color(\n            \"Total:N\",\n            legend=alt.Legend(orient=\"top-left\", title=\"\")\n        ),\n        alt.Order(\"Total:N\")\n    )\n    .properties(\n        width=PLOT_WIDTH,\n        height=PLOT_HEIGHT,\n        title=\"Monthly Budget\"\n    )\n)\n\nchart.show()\n\n\n\n\n\n\n\n\nUsing the Net Income result, I can estimate the probability that the household will lose money in a given month:\n\n\nCode\np_lose_money = float(np.round(np.mean(df[\"Net Income\"] &lt;= 0) * 100, 2))\n\n\nThis tells us that the household has a 23.12% chance of losing money each month, and that Audrey should probably get a divorce."
  },
  {
    "objectID": "posts/spending/index.html#overall-budget",
    "href": "posts/spending/index.html#overall-budget",
    "title": "Amy, Brett and the Cost of Living Crisis",
    "section": "Overall Budget",
    "text": "Overall Budget\nNow I can plug the above simulated numbers into the formula:\n\\[\nC_{\\text{total}} = 7500 + (30(A + B))\n\\]\nto arrive at the final monthly total.\nAssuming Amy makes $550 per day worked after tax, I can also compute a monthly net income.\n\n\nCode\ndef calculate_total_monthly_expenses(\n    audrey_spend, \n    brett_spend, \n    base_expense: float = 7500\n) -&gt; np.ndarray:\n    audrey_total_spend = np.sum(audrey_spend, axis=1)\n    brett_total_spend = np.sum(brett_spend, axis=1)\n\n    return base_expense + audrey_total_spend + brett_total_spend\n\ntotal_expenses = calculate_total_monthly_expenses(\n    audrey_spend=simdata_a,\n    brett_spend=simdata_b,\n    base_expense=7500\n)\n\nincome_per_day = 550\nincome = np.sum(\n    np.where(\n        ~spending_days[:, :, 0],\n        income_per_day, \n        0\n    ),\n    axis=1\n)\n\ndf = pd.DataFrame({\n    \"Expenses\": total_expenses,\n    \"Income\": income,\n    \"Net Income\": income - total_expenses\n})\nflat_df = df.melt(var_name=\"Total\", value_name=\"Amount\")\nflat_df = flat_df.sample(5000)\n\nchart = (\n    alt.Chart(flat_df)\n    .mark_bar(opacity=0.5)\n    .encode(\n        alt.X(\"Amount:Q\", bin=alt.Bin(maxbins=50)),\n        alt.Y(\n            \"count():Q\", \n            stack=None,\n            title='',\n            axis=alt.Axis(\n                labels=False, \n                ticks=False, \n                grid=False, \n                domain=False\n            ),\n        ),\n        alt.Color(\n            \"Total:N\",\n            legend=alt.Legend(orient=\"top-left\", title=\"\")\n        ),\n        alt.Order(\"Total:N\")\n    )\n    .properties(\n        width=PLOT_WIDTH,\n        height=PLOT_HEIGHT,\n        title=\"Monthly Budget\"\n    )\n)\n\nchart.show()\n\n\n\n\n\n\n\n\nUsing the Net Income result, I can estimate the probability that the household will lose money in a given month:\n\n\nCode\np_lose_money = float(np.round(np.mean(df[\"Net Income\"] &lt;= 0) * 100, 2))\n\n\nThis tells us that the household has a 32.64% chance of losing money each month, and that Amy should probably get a divorce."
  },
  {
    "objectID": "posts/spending/index.html#amy",
    "href": "posts/spending/index.html#amy",
    "title": "Amy, Brett and the Cost of Living Crisis",
    "section": "Amy",
    "text": "Amy\n\nAmy is a freelancer and goes to work on 85% of days, where she spends no money\nOn days she is not working, she spends anywhere between $20 and $150\n\n\\[\nA \\sim\n\\begin{cases}\n0, & \\text{with probability } 0.15 \\\\\nC_A \\sim \\text{PERT}(20, 50, 150), & \\text{with probability } 0.85\n\\end{cases}\n\\]"
  },
  {
    "objectID": "posts/dirichlet_geo/index.html",
    "href": "posts/dirichlet_geo/index.html",
    "title": "Bayesian Geology",
    "section": "",
    "text": "El Gordo, Tabernas Basin, Spain"
  },
  {
    "objectID": "posts/dirichlet_geo/index.html#the-dirichlet-distribution",
    "href": "posts/dirichlet_geo/index.html#the-dirichlet-distribution",
    "title": "Interpreting Depositional Environments with Bayes",
    "section": "The Dirichlet Distribution",
    "text": "The Dirichlet Distribution\nThe Dirichet distribution can be used to model the probability of a set of categories, which in this context would be depositional environments. The Dirichlet distribution samples from a simplex (a vector of numbers that add to 1), which makes it perfect for modelling our uncertainty over interpretations.\nIt also has the convenient property of conjugacy, which essentially means its very easy to go from prior to posterior, which will be an important part of this process."
  },
  {
    "objectID": "posts/dirichlet_geo/index.html#applied-to-a-simple-example",
    "href": "posts/dirichlet_geo/index.html#applied-to-a-simple-example",
    "title": "Interpreting Depositional Environments with Bayes",
    "section": "Applied to a Simple Example",
    "text": "Applied to a Simple Example\nLet’s say we are considering three depositional environments when looking at part of an outcrop:\n\nDelta\nDeepwater turbidite fan\nLake\n\nThe \\(\\alpha\\) parameter of the distribution can be thought of as a list of weights, one for each option.\nIn this case let’s say we assign our weights as:\n\\[\n\\alpha = [50, 25, 1]\n\\]\nThis equates to us saying that we think the Delta interpretation is most plausible, although the Turbidite interpretation is a possibility, while the Lake interpretation is implausible.\nThat results in the following distribution:\n\n\nCode\nsims = 1000\nalpha = [50, 25, 1]\n\nsimdata = np.random.dirichlet(alpha, size=sims)\n\ndf = pd.DataFrame({\n    \"P(Delta)\": simdata[:, 0],\n    \"P(Turbidite)\": simdata[:, 1],\n    \"P(Lake)\": simdata[:, 2],\n}).melt(var_name=\"Interpretation\", value_name=\"Probability\")\n\ndef plot_dirichlet_histogram(df: pd.DataFrame, title: str) -&gt; alt.Chart:\n    tick_values = [round(i * 0.1, 1) for i in range(11)]\n    return (\n        alt.Chart(df)\n        .mark_bar(opacity=0.5)\n        .encode(\n            alt.X(\n                \"Probability:Q\", \n                bin=alt.Bin(maxbins=100),\n                scale=alt.Scale(domain=[0, 1]),\n                axis=alt.Axis(values=tick_values)\n            ),\n            alt.Y(\n                \"count():Q\", \n                stack=None,\n                title='',\n                axis=alt.Axis(\n                    labels=False, \n                    ticks=False, \n                    grid=False, \n                    domain=False\n                )\n            ),\n\n            alt.Color(\n                \"Interpretation:N\",\n                legend=alt.Legend(orient=\"top-right\")\n            ),\n            alt.Order(\"Interpretation:N\")\n        )\n        .properties(\n            title=title,\n            height=PLOT_HEIGHT,\n            width=PLOT_WIDTH\n        )\n    )\n\nchart = plot_dirichlet_histogram(df, title=\"Interpretation Probabilities\")\nchart.show()\n\n\n\n\n\n\n\n\nThis is the distribution over the probabilities of each of the interpretations. Maybe it seems weird to have a probability distribution over probabilities, but this is actually very common in bayesian statistics. I will write a post on that at some point."
  },
  {
    "objectID": "posts/dirichlet_geo/index.html#incorporating-prior-information",
    "href": "posts/dirichlet_geo/index.html#incorporating-prior-information",
    "title": "Interpreting Depositional Environments with Bayes",
    "section": "Incorporating Prior Information",
    "text": "Incorporating Prior Information\nLet’s say despite our observations at this outcrop, we have very strong indications nearby that we are standing in the middle of a lake deposit. Maybe the outcrop is an anomaly in an otherwise fairly confident interpretation.\nIf we encode that prior as \\(\\alpha_{\\text{prior}} = [1, 1, 100]\\), it looks like this:\n\n\nCode\nprior_alpha = np.array([1, 1, 100])\nsimdata = np.random.dirichlet(prior_alpha, size=sims)\n\ndf = pd.DataFrame({\n    \"P(Delta)\": simdata[:, 0],\n    \"P(Turbidite)\": simdata[:, 1],\n    \"P(Lake)\": simdata[:, 2],\n}).melt(var_name=\"Interpretation\", value_name=\"Probability\")\n\nchart = plot_dirichlet_histogram(df, title=\"Prior Interpretation Probabilities\")\nchart.show()\n\n\n\n\n\n\n\n\nWe can incorporate this prior information very easily in our \\(\\alpha\\) parameter by just… adding it! It seems like a dumb trick, but this is an analytical result for updating Dirichlet distributions in what’s called a Dirichlet Multinomial conjugate model.\n\\[\n\\begin{align}\n\\alpha_{\\text{prior}} &= [1, 1, 200] \\\\\n\\alpha_{\\text{posterior}} &= \\alpha_{\\text{prior}} + \\alpha \\\\\n\\alpha_{\\text{posterior}} &= [51, 26, 201]\n\\end{align}\n\\]\n\n\nCode\nprior_alpha = np.array([1, 1, 200])\nalpha = np.array([50, 25, 1])\nposterior_alpha = prior_alpha + alpha\n\nsimdata = np.random.dirichlet(posterior_alpha, size=sims)\n\ndf = pd.DataFrame({\n    \"P(Delta)\": simdata[:, 0],\n    \"P(Turbidite)\": simdata[:, 1],\n    \"P(Lake)\": simdata[:, 2],\n}).melt(var_name=\"Interpretation\", value_name=\"Probability\")\n\nchart = plot_dirichlet_histogram(df, title=\"Posterior Interpretation Probabilities\")\nchart.show()\n\n\n\n\n\n\n\n\nNow, the strong prior on the Lake interpretation pushes it ahead, but notice the other interpretations are still on the table as possibilities given the evidence observed at the outcrop.\nIn my view this provides a pretty strong baseline. We have 3 important things:\n\nThe ability to encode prior beliefs\nThe ability to “score” interpretations based on their likelihoods\nThe ability to update probabilities objectively\n\nThis all hinges on a point-scoring system that makes logical sense, but it seems like a good start."
  },
  {
    "objectID": "posts/dirichlet_geo/index.html#in-action",
    "href": "posts/dirichlet_geo/index.html#in-action",
    "title": "Interpreting Depositional Environments with Bayes",
    "section": "In Action",
    "text": "In Action\nI don’t have real examples to use, but let’s consider a diligent field geologist’s notes about an outcrop we are considering. For now let’s consider one section of the outcrop that the geo has annotated.\nSince Claude is cooking, I’ll use it to dream up this scenario:\n\nFIELD NOTES - Station 15, Muddy Creek Section Date: June 15, 2024 Weather: Overcast, good exposure Geologist: Dr. Sarah Mitchell\n\n\nINTERVAL: 45.2 - 52.8m (Sandstone Unit C)\n\n\nDETAILED OBSERVATIONS: Trough cross-stratification: Observed in 7 separate beds, sets 0.3-1.2m thick. Parallel lamination: Very common, counted 12 distinct intervals. Massive sandstone: 3 thick beds (0.8-1.5m), clean, well-sorted. Plant debris: Abundant! Counted 15 fragments/coalified pieces on bedding planes. Ripple lamination: Present in 4 beds, mostly at tops of fining-up sequences. Fining upward sequences: Clear in 5 complete cycles, 2-3m thick each. Channel lag deposits: 2 clear examples at sequence bases, pebble lags. Mud drapes: Rare, only 1 thin example on ripple surface. Bioturbation: Moderate, 6 intervals with Skolithos-type traces.\n\n\nINTERPRETATION NOTES: This interval screams fluvial-deltaic to me. The trough cross-beds in thick sets, abundant plant material, and those beautiful fining-upward cycles. The channel lags are textbook. Probably distributary channel fill transitioning upward to delta front. Very little marine influence based on lack of mud drapes and bioturbation style.\n\n\nCONFIDENCE: High - excellent exposure, clear diagnostic features\n\nClaude is… quite good at this.\nWrapping this up into data, we can calculate the posterior \\(\\alpha\\) values using the previous table.\n\n\nCode\nfield_observations = {\n    'trough_cross_stratification': 7,\n    'parallel_lamination': 12,\n    'massive_sandstone': 3,\n    'plant_debris': 15,\n    'ripple_lamination': 4,\n    'fining_upward_sequences': 5,\n    'channel_lag_deposits': 2,\n    'mud_drapes': 1,\n    'bioturbation': 6\n}\n\n\ndef calculate_posterior_alphas(\n    obs_df: pd.DataFrame, \n    field_observations: dict[str, int]\n):\n    alpha_counts = {k: 0 for k in obs_df.columns if k != \"observation\"}\n\n    for obs, count in field_observations.items():\n        obs_row = obs_df[obs_df['observation'] == obs]\n\n        for alpha in alpha_counts.keys():\n            a = obs_row[alpha].values[0]\n            alpha_counts[alpha] += (a * count)\n\n    return alpha_counts\n\nalpha_dict = calculate_posterior_alphas(obs_df, field_observations)\n\nalpha_obs = np.array(list(alpha_dict.values()))\n\n\nAnd then plot it:\n\n\nCode\nsimdata = np.random.dirichlet(alpha_obs, size=sims)\n\ndf = pd.DataFrame({\n    \"P(Delta)\": simdata[:, 0],\n    \"P(Turbidite)\": simdata[:, 1],\n    \"P(Lake)\": simdata[:, 2],\n}).melt(var_name=\"Interpretation\", value_name=\"Probability\")\n\nchart = plot_dirichlet_histogram(df, title=\"Field Observations: Probabilities\")\nchart.show()\n\n\n\n\n\n\n\n\nDr. Mitchell’s remarks were broadly correct, then, though the model only gives about 50% probability to the Delta interpretation."
  },
  {
    "objectID": "posts/dirichlet_geo/index.html#incorporating-other-information",
    "href": "posts/dirichlet_geo/index.html#incorporating-other-information",
    "title": "Interpreting Depositional Environments with Bayes",
    "section": "Incorporating Other Information",
    "text": "Incorporating Other Information\nIn the same way we incorporated priors before, we can use the same procedure to incorporate other information. For example, the geophysicist working on this dataset may have a strong conviction that this is a turbidite deposit based on interpretation of seismic data nearby that connects to this outcrop stratigraphically:\n\\[\n\\begin{align}\n\\alpha_{\\text{geophysics}} = [1, 500, 1]\n\\end{align}\n\\]\n\n\nCode\nnew_alpha = np.array(alpha_obs) + np.array([1, 500, 1])\nsimdata = np.random.dirichlet(new_alpha, size=sims)\n\ndf = pd.DataFrame({\n    \"P(Delta)\": simdata[:, 0],\n    \"P(Turbidite)\": simdata[:, 1],\n    \"P(Lake)\": simdata[:, 2],\n}).melt(var_name=\"Interpretation\", value_name=\"Probability\")\n\nchart = plot_dirichlet_histogram(\n    df, \n    title=\"Field Observations + Geophysics: Probabilities\"\n)\nchart.show()"
  }
]