[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "What is this blog for?",
    "section": "",
    "text": "I’m making this blog to better organize my thoughts, my work portfolio and to develop my writing skills, mostly through technical sharing. It will probably contain writing on statistics, programming and the occasional book review."
  },
  {
    "objectID": "posts/outcomes/index.html",
    "href": "posts/outcomes/index.html",
    "title": "Correlated Outcomes",
    "section": "",
    "text": "In the previous post I explained how Copulas can be used to sample distributions with correlation. A nice extention of that logic is to simulate discrete outcomes with correlation as well.\n\n\nThe modelling of independent and identically distributed trials is easily done with the Binomial distribution. For example, we can simulate tossing a coin 50 times like this:\n\n\nCode\n# Simulate 5000 experiments, tossing a coin 50 times\ntosses = 50\nn = 5000\n\n# Assume a fair coin\np = 0.5\n\nheads = np.random.binomial(tosses, p, n)\n\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nx_domain = [0, tosses]\ndef plot_disrete_histogram(df, col: str, color: str = \"dodgerblue\") -&gt; alt.Chart:\n    tick_values = list(range(0, tosses + 1, tosses // 10))\n\n    return (\n        alt.Chart(df)\n        .mark_bar(color=color, opacity=0.75)\n        .encode(\n            alt.X(\n                f\"{col}:Q\", \n                bin=alt.Bin(step=1), \n                scale=alt.Scale(domain=[0, tosses]),\n                axis=alt.Axis(values=tick_values, format=\".0f\")\n            ),\n            alt.Y(\"count():Q\")\n        )\n        .properties(\n            height=PLOT_HEIGHT,\n            width=PLOT_WIDTH\n        )\n    )\n\nplot_disrete_histogram(df,\"Heads\").show()\n\n\n\n\n\n\n\n\nThe above plot shows the number of heads thrown out of 50 total tosses, repeated 5000 times.\nThis can be made a bit more flexible by generating our own random uniform samples instead of directly using numpy’s random.binomial function:\n\n\nCode\n# Simulate 2D array of 0-1 values with dims (tosses, n)\nuniform_simdata = np.random.uniform(0, 1, size=(tosses, n))\n\n# Convert to a boolean array based on the p value\noutcomes = np.where(\n    uniform_simdata &lt; p,\n    True,\n    False\n)\n\n# Sum along the tosses axis to get total heads per experiment\nheads = np.sum(outcomes, axis=0)\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nplot_disrete_histogram(df, \"Heads\", color=\"coral\").show()\n\n\n\n\n\n\n\n\nThis might seem a bit inconvenient, but when simulating correlated events it becomes an effective way of getting the desired result.\n\n\n\nBecause we are now using uniform samples to create the outcomes, we can just feed correlated uniform samples into the process instead. This will correlate the outcomes.\nHere is a correlation of \\(0.25\\) shared across all coin tosses:\n\n\nCode\ndef create_simple_correlation_matrix(num: int, corr: float) -&gt; np.ndarray:\n    matrix = np.full((num, num), corr)\n    np.fill_diagonal(matrix, 1)\n    return matrix\n\ncorr = create_simple_correlation_matrix(num=tosses, corr=0.25)\n\n# Use a copula to generate correlated uniform samples\nsamples = stats.norm.cdf(\n    np.random.multivariate_normal(\n        mean=np.zeros(tosses),\n        cov=corr,\n        size=n\n    )\n).T\n\noutcomes = np.where(\n    samples &lt; p,\n    True,\n    False\n)\n\nheads = np.sum(outcomes, axis=0)\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nplot_disrete_histogram(df, \"Heads\", color=\"orange\").show()\n\n\n\n\n\n\n\n\nAnd another with a correlation of 0.75:\n\n\nCode\ncorr = create_simple_correlation_matrix(num=tosses, corr=0.75)\n\n# Use a copula to generate correlated uniform samples\nsamples = stats.norm.cdf(\n    np.random.multivariate_normal(\n        mean=np.zeros(tosses),\n        cov=corr,\n        size=n\n    )\n).T\n\noutcomes = np.where(\n    samples &lt; p,\n    True,\n    False\n)\n\nheads = np.sum(outcomes, axis=0)\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nplot_disrete_histogram(df, \"Heads\", color=\"yellow\").show()\n\n\n\n\n\n\n\n\nThe correlation between events has the effect of bifurcating the results, because events are firing together more often. A correlation of \\(1\\) using this method would return 0 heads or 100 heads, with nothing in between.\nThese examples are using a simple correlation matrix filled with the same value, but richer correlation structures can be modelled using covariance functions. The correlations might vary through time or space (or both), which means trials closer together may be more tightly related, for example. This would lead to sampling from a “binomial process” rather than the regular binomial distribution.\n\n\n\nThe framework outlined in this post and the previous provides a toolkit for numerically simulating many 0-dimensional problems (one where the answer is one number, not a higher dimensional array like a 2D map or 3D grid). Because these problems in simulation return a 1D array of results, I often refer to them as 1D models, but this can be a confusing term as the input space may have many dimensions – 1D refers only to the output space.\nMany real world problems boil down to what are called Zero-inflated models, where an event occurs with some probability \\(p\\), returning a random variable, or else returns zero:\n\\[\nX \\sim\n\\begin{cases}\n0, & \\text{with probability } 1 - p \\\\\nY \\sim \\mathcal{D}(\\theta), & \\text{with probability } p\n\\end{cases}\n\\]\nI’ll make things a bit more concrete with an example in my next post."
  },
  {
    "objectID": "posts/outcomes/index.html#independent-events",
    "href": "posts/outcomes/index.html#independent-events",
    "title": "Correlated Outcomes",
    "section": "",
    "text": "The modelling of independent and identically distributed trials is easily done with the Binomial distribution. For example, we can simulate tossing a coin 50 times like this:\n\n\nCode\n# Simulate 5000 experiments, tossing a coin 50 times\ntosses = 50\nn = 5000\n\n# Assume a fair coin\np = 0.5\n\nheads = np.random.binomial(tosses, p, n)\n\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nx_domain = [0, tosses]\ndef plot_disrete_histogram(df, col: str, color: str = \"dodgerblue\") -&gt; alt.Chart:\n    tick_values = list(range(0, tosses + 1, tosses // 10))\n\n    return (\n        alt.Chart(df)\n        .mark_bar(color=color, opacity=0.75)\n        .encode(\n            alt.X(\n                f\"{col}:Q\", \n                bin=alt.Bin(step=1), \n                scale=alt.Scale(domain=[0, tosses]),\n                axis=alt.Axis(values=tick_values, format=\".0f\")\n            ),\n            alt.Y(\"count():Q\")\n        )\n        .properties(\n            height=PLOT_HEIGHT,\n            width=PLOT_WIDTH\n        )\n    )\n\nplot_disrete_histogram(df,\"Heads\").show()\n\n\n\n\n\n\n\n\nThe above plot shows the number of heads thrown out of 50 total tosses, repeated 5000 times.\nThis can be made a bit more flexible by generating our own random uniform samples instead of directly using numpy’s random.binomial function:\n\n\nCode\n# Simulate 2D array of 0-1 values with dims (tosses, n)\nuniform_simdata = np.random.uniform(0, 1, size=(tosses, n))\n\n# Convert to a boolean array based on the p value\noutcomes = np.where(\n    uniform_simdata &lt; p,\n    True,\n    False\n)\n\n# Sum along the tosses axis to get total heads per experiment\nheads = np.sum(outcomes, axis=0)\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nplot_disrete_histogram(df, \"Heads\", color=\"coral\").show()\n\n\n\n\n\n\n\n\nThis might seem a bit inconvenient, but when simulating correlated events it becomes an effective way of getting the desired result."
  },
  {
    "objectID": "posts/outcomes/index.html#correlated-events",
    "href": "posts/outcomes/index.html#correlated-events",
    "title": "Correlated Outcomes",
    "section": "",
    "text": "Because we are now using uniform samples to create the outcomes, we can just feed correlated uniform samples into the process instead. This will correlate the outcomes.\nHere is a correlation of \\(0.25\\) shared across all coin tosses:\n\n\nCode\ndef create_simple_correlation_matrix(num: int, corr: float) -&gt; np.ndarray:\n    matrix = np.full((num, num), corr)\n    np.fill_diagonal(matrix, 1)\n    return matrix\n\ncorr = create_simple_correlation_matrix(num=tosses, corr=0.25)\n\n# Use a copula to generate correlated uniform samples\nsamples = stats.norm.cdf(\n    np.random.multivariate_normal(\n        mean=np.zeros(tosses),\n        cov=corr,\n        size=n\n    )\n).T\n\noutcomes = np.where(\n    samples &lt; p,\n    True,\n    False\n)\n\nheads = np.sum(outcomes, axis=0)\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nplot_disrete_histogram(df, \"Heads\", color=\"orange\").show()\n\n\n\n\n\n\n\n\nAnd another with a correlation of 0.75:\n\n\nCode\ncorr = create_simple_correlation_matrix(num=tosses, corr=0.75)\n\n# Use a copula to generate correlated uniform samples\nsamples = stats.norm.cdf(\n    np.random.multivariate_normal(\n        mean=np.zeros(tosses),\n        cov=corr,\n        size=n\n    )\n).T\n\noutcomes = np.where(\n    samples &lt; p,\n    True,\n    False\n)\n\nheads = np.sum(outcomes, axis=0)\ndf = pd.DataFrame({\n    \"Heads\": heads\n})\n\nplot_disrete_histogram(df, \"Heads\", color=\"yellow\").show()\n\n\n\n\n\n\n\n\nThe correlation between events has the effect of bifurcating the results, because events are firing together more often. A correlation of \\(1\\) using this method would return 0 heads or 100 heads, with nothing in between.\nThese examples are using a simple correlation matrix filled with the same value, but richer correlation structures can be modelled using covariance functions. The correlations might vary through time or space (or both), which means trials closer together may be more tightly related, for example. This would lead to sampling from a “binomial process” rather than the regular binomial distribution."
  },
  {
    "objectID": "posts/outcomes/index.html#bringing-things-together",
    "href": "posts/outcomes/index.html#bringing-things-together",
    "title": "Correlated Outcomes",
    "section": "",
    "text": "The framework outlined in this post and the previous provides a toolkit for numerically simulating many 0-dimensional problems (one where the answer is one number, not a higher dimensional array like a 2D map or 3D grid). Because these problems in simulation return a 1D array of results, I often refer to them as 1D models, but this can be a confusing term as the input space may have many dimensions – 1D refers only to the output space.\nMany real world problems boil down to what are called Zero-inflated models, where an event occurs with some probability \\(p\\), returning a random variable, or else returns zero:\n\\[\nX \\sim\n\\begin{cases}\n0, & \\text{with probability } 1 - p \\\\\nY \\sim \\mathcal{D}(\\theta), & \\text{with probability } p\n\\end{cases}\n\\]\nI’ll make things a bit more concrete with an example in my next post."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Nicholas Dorsch - Geoscientist/Data Analyist\nI come from a geology and geophysics background working in the oil and gas industry, but over the years have developed a passion for bayesian statistics and data science more broadly. My skillset includes a mixture of geology and geophysics workflows as well as more general purpose statistical modelling. The intersection of these things has allowed me to work on fairly unique projects during my career.\nI strongly believe that bayesian statistical models offer the most robust and flexible framework for uncertainty quantification, as well as decisions making under uncertainty. For that reason they have become my career focus.\nMore recently I have developed more data engineering skills, working with database design and management. My goal is to become a “full stack” data scientist, capable of managing and deploying analytical processes end to end, to help organisations make the best use of their data.\n\n\nContact information\nbased in: Melbourne, Australia\navailability: currently a full time employee of Karoon Energy\nemail: nickjdorsch@gmail.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Correlated Outcomes\n\n\n\ncode\n\nstatistics\n\n\n\n\n\n\n\n\n\nJun 1, 2025\n\n\nNicholas Dorsch\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelated Sampling Made Easy\n\n\n\ncode\n\nstatistics\n\n\n\n\n\n\n\n\n\nMay 31, 2025\n\n\nNicholas Dorsch\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is this blog for?\n\n\n\nupdates\n\n\n\n\n\n\n\n\n\nMay 28, 2025\n\n\nNicholas Dorsch\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/copulas/index.html",
    "href": "posts/copulas/index.html",
    "title": "Correlated Sampling Made Easy",
    "section": "",
    "text": "A common “gotcha” with creating simple Monte Carlo scripts in Python or Excel is the request to add correlation to the distributions being sampled. This is a difficult thing to do in Excel without using plugins like AtRisk, but in Python, correlated sampling can be performed conveniently with the use of Copulas.\nThe fundamental intuition to build when working with copulas is that all continuous distributions can be remapped to a \\(\\text{Uniform}[0, 1]\\) distribution with their Cumulative Distribution Function (CDF), and transformed back with the inverse CDF, also called the Percent Point Function (PPF). Furthermore, sampling from any distribution can be done by running uniform samples through its PPF (that is how random variate sampling is done in general).\nWith that trick in mind, we can make use of the multivariate Gaussian distribution and its CDF to easily generate correlated samples of any distributions we want, which is very useful when creating numerical simulation models."
  },
  {
    "objectID": "posts/copulas/index.html#generate-correlated-samples",
    "href": "posts/copulas/index.html#generate-correlated-samples",
    "title": "Correlated Sampling Made Easy",
    "section": "Generate Correlated Samples",
    "text": "Generate Correlated Samples\nLet’s say we have three parameters \\(a\\), \\(b\\) and \\(c\\) that we want to sample with some correlation coefficient \\(\\rho\\). Before we worry about the specifics of those distributions, we can sample from a multivariate Gaussian with that correlation structure.\n\n\nCode\n# Number of samples\nn = 2500\n\n# Correlation matrix\nrho = 0.80\ncorr = np.array([\n    [1.0, rho, rho],\n    [rho, 1.0, rho],\n    [rho, rho, 1.0],\n])\n\n# Mean vector (zeros with standard mv normal)\nmu = [0.0, 0.0, 0.0]\n\n# Generate samples with dims (n, parameter)\nsamples = np.random.multivariate_normal(\n    mean=mu,\n    cov=corr,\n    size=n\n)\n\n\nThis generates 2500 samples from the standard multivariate Gaussian distribution with correlation \\(\\rho = 0.8\\).\n\n\nCode\ndf = pd.DataFrame({\n    \"a\": samples[:, 0],\n    \"b\": samples[:, 1],\n    \"c\": samples[:, 2],\n})\n\ndef create_pairplot(df: pd.DataFrame, color: str = \"dodgerblue\") -&gt; alt.RepeatChart:\n    return (\n        alt.Chart(df)\n        .mark_circle(\n            color=color,\n            opacity=0.25\n        )\n        .encode(\n            alt.X(\n                alt.repeat(\"column\"), \n                type=\"quantitative\", \n                scale=alt.Scale(zero=False)\n            ),\n            alt.Y(\n                alt.repeat(\"row\"), \n                type=\"quantitative\", \n                scale=alt.Scale(zero=False)\n            )\n        )\n        .properties(\n            height=PLOT_HEIGHT / 3,\n            width=180,\n        )\n        .repeat(\n            row=list(df.columns),\n            column=list(df.columns)\n        )\n    )\n\ncreate_pairplot(df, color=\"lightcoral\")\n\n\n\n\n\n\n\n\n\n\nCode\ndf.corr()\n\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\na\n1.000000\n0.801207\n0.799221\n\n\nb\n0.801207\n1.000000\n0.798064\n\n\nc\n0.799221\n0.798064\n1.000000"
  },
  {
    "objectID": "posts/copulas/index.html#transform-to-uniform",
    "href": "posts/copulas/index.html#transform-to-uniform",
    "title": "Correlated Sampling Made Easy",
    "section": "Transform to Uniform",
    "text": "Transform to Uniform\nWe can transform those samples to \\(\\text{Uniform} [0, 1]\\) by simply passing them through the standard normal CDF.\n\n\nCode\nuniform_samples = stats.norm.cdf(df.values)\nuniform_df = pd.DataFrame({\n    \"a\": uniform_samples[:, 0],\n    \"b\": uniform_samples[:, 1],\n    \"c\": uniform_samples[:, 2],\n})\n\ncreate_pairplot(uniform_df, color=\"pink\")\n\n\n\n\n\n\n\n\nNow the sample are transformed to the uniform domain, but importantly their correlation structure has been preserved.\n\n\nCode\nuniform_df.corr()\n\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\na\n1.000000\n0.790176\n0.790012\n\n\nb\n0.790176\n1.000000\n0.786882\n\n\nc\n0.790012\n0.786882\n1.000000"
  },
  {
    "objectID": "posts/copulas/index.html#transform-to-distributions",
    "href": "posts/copulas/index.html#transform-to-distributions",
    "title": "Correlated Sampling Made Easy",
    "section": "Transform to Distributions",
    "text": "Transform to Distributions\nNow we have a bunch of uniformly distributed random samples that have our desired correlation structure. All that is left is to map the samples to our desired distributions using their respective PPFs.\nLet’s define our distributions as:\n\\[\n\\begin{align}\nA &\\sim \\text{Normal}(500, 50) \\\\\nB &\\sim \\text{Gamma}(2, 5) \\\\\nC &\\sim \\text{Beta}(5, 8)\n\\end{align}\n\\]\nNotice I’m not using normal distribtions exclusively, I can use the copula to map to whatever distributions I want. Note only that that the more skewed and kurtotic the distributions, the more warping will occur in their correlations out the other end. There are other types of Copula that can handle this better than the Gaussian Copula used here.\n\n\nCode\ndist_df = pd.DataFrame({\n    \"a\": stats.norm(500, 25).ppf(uniform_df[\"a\"]),\n    \"b\": stats.gamma(a=2, scale=5, loc=0).ppf(uniform_df[\"b\"]),\n    \"c\": stats.beta(5, 8).ppf(uniform_df[\"c\"])\n})\n\ncreate_pairplot(dist_df, color=\"orange\")\n\n\n\n\n\n\n\n\n\n\nCode\ndist_df.corr()\n\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\na\n1.000000\n0.759275\n0.797616\n\n\nb\n0.759275\n1.000000\n0.769888\n\n\nc\n0.797616\n0.769888\n1.000000"
  },
  {
    "objectID": "posts/copulas/index.html#comparison-with-uncorrelated-sampling",
    "href": "posts/copulas/index.html#comparison-with-uncorrelated-sampling",
    "title": "Correlated Sampling Made Easy",
    "section": "Comparison with Uncorrelated Sampling",
    "text": "Comparison with Uncorrelated Sampling\nIf we skip the Copula and just sample from our distributions, we get this:\n\n\nCode\nunc_dist_df = pd.DataFrame({\n    \"a\": stats.norm(500, 25).rvs(size=n),\n    \"b\": stats.gamma(a=2, scale=5, loc=0).rvs(size=n),\n    \"c\": stats.beta(5, 8).rvs(size=n)\n})\n\ncreate_pairplot(unc_dist_df, color=\"lightblue\")\n\n\n\n\n\n\n\n\n\n\nCode\nunc_dist_df.corr()\n\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\na\n1.000000\n0.031195\n0.012240\n\n\nb\n0.031195\n1.000000\n-0.024909\n\n\nc\n0.012240\n-0.024909\n1.000000\n\n\n\n\n\n\n\n\nEffect on Monte Carlo Simulation\nLet’s say that whatever these parameters represent, we want to know the result of this expression:\n\\[\ny = ab^c\n\\]\n\n\nCode\nunc_dist_df[\"y\"] = unc_dist_df[\"a\"] * unc_dist_df[\"b\"] ** unc_dist_df[\"c\"]\ndist_df[\"y\"] = dist_df[\"a\"] * dist_df[\"b\"] ** dist_df[\"c\"]\n\nunc_dist_df[\"Case\"] = \"Uncorrelated\"\ndist_df[\"Case\"] = \"Correlated\"\n\ncombined_df = pd.concat([dist_df, unc_dist_df], ignore_index=True)\n\n# Filter tail\nupper_lim = np.percentile(combined_df[\"y\"], q=99.5)\ncombined_df = combined_df[\n    combined_df[\"y\"] &lt; upper_lim\n]\n\nchart = (\n    alt.Chart(combined_df)\n    .mark_bar(opacity=0.75)\n    .encode(\n        x=alt.X(\"y:Q\", bin=alt.Bin(maxbins=50), axis=alt.Axis(title=\"y\")),\n        y=alt.Y(\n            \"count():Q\", \n            axis=alt.Axis(labels=False, ticks=False, title=None),\n            stack=False\n        ),\n        color=alt.Color(\n            \"Case:N\", \n            scale=alt.Scale(\n                domain=[\"Correlated\", \"Uncorrelated\"], \n                range=[\"orange\", \"lightblue\"]\n            )\n        )\n    )\n    .properties(height=PLOT_HEIGHT, width=PLOT_WIDTH)\n)\n\n\nchart.show()\n\n\n\n\n\n\n\n\n\n\nCode\npercentiles = [10, 50, 90]\n\nresult_df = pd.DataFrame({\n    \"10th\": np.round(np.percentile(dist_df[\"y\"], q=percentiles), 3),\n    \"50th\": np.round(np.percentile(unc_dist_df[\"y\"], q=percentiles), 3)\n}).T\n\n# Fix column labels\nresult_df.columns = [f\"P{p}\" for p in percentiles]\nresult_df.index = [\"Correlated\", \"Uncorrelated\"]\n\nresult_df\n\n\n\n\n\n\n\n\n\nP10\nP50\nP90\n\n\n\n\nCorrelated\n590.477\n1089.344\n2625.032\n\n\nUncorrelated\n673.460\n1069.208\n1937.377\n\n\n\n\n\n\n\nThe correlated samples show a wider uncertainty range in the result than the uncorrelated samples, as is expected. This may be an important detail to capture depending on the sensitivity of the analysis. The effect of correlation can also be quite unintuitive, so it is always worth checking the effect it has on results."
  },
  {
    "objectID": "posts/copulas/index.html#summary-of-process",
    "href": "posts/copulas/index.html#summary-of-process",
    "title": "Correlated Sampling Made Easy",
    "section": "Summary of Process",
    "text": "Summary of Process\n\n\n\n\n\ngraph TD\n    corr[\"Correlation Matrix\"]\n    mvn[\"Standard MvNorm(0, corr)\"]\n\n    corr --&gt; mvn --&gt; mvn_samples\n\n    mvn_samples[\"MvNorm Samples [2500, 3]\"]\n    uniform[\"Uniform Samples [2500, 3]\"]\n\n    mvn_samples --&gt;|Normal CDF| uniform\n\n    dist_a[\"a\"]\n    dist_b[\"b\"]\n    dist_c[\"c\"]\n\n    dist_a_samples[\"a Samples [2500]\"]\n    dist_b_samples[\"b Samples [2500]\"]\n    dist_c_samples[\"c Samples [2500]\"]\n\n    dist_a ---&gt;|PPF| dist_a_samples\n    dist_b ---&gt;|PPF| dist_b_samples\n    dist_c ---&gt;|PPF| dist_c_samples\n\n    uniform --&gt; dist_a_samples\n    uniform --&gt; dist_b_samples\n    uniform --&gt; dist_c_samples"
  }
]